{
  "active": true,
  "activeVersionId": "8d9a6cbf-e00e-4f81-a475-6a7def9ccd57",
  "connections": {
    "Build SQL INSERT": {
      "main": [
        [
          {
            "index": 0,
            "node": "Postgres INSERT",
            "type": "main"
          }
        ]
      ]
    },
    "Call 'PKM — Retrieval Config'": {
      "main": [
        [
          {
            "index": 0,
            "node": "Capture",
            "type": "main"
          }
        ]
      ]
    },
    "Capture": {
      "main": [
        [
          {
            "index": 0,
            "node": "Switch",
            "type": "main"
          }
        ]
      ]
    },
    "Compose Reply Text": {
      "main": [
        [
          {
            "index": 0,
            "node": "Send a text message",
            "type": "main"
          }
        ]
      ]
    },
    "Compute Retrieval Excerpt + Quality (email clean_text)": {
      "main": [
        [
          {
            "index": 0,
            "node": "Build SQL INSERT",
            "type": "main"
          }
        ]
      ]
    },
    "Create Sampled Prompt": {
      "main": [
        [
          {
            "index": 0,
            "node": "Message a model",
            "type": "main"
          }
        ]
      ]
    },
    "Create Whole Text Prompt": {
      "main": [
        [
          {
            "index": 0,
            "node": "Message a model",
            "type": "main"
          }
        ]
      ]
    },
    "Email Trigger (IMAP)1": {
      "main": [
        [
          {
            "index": 0,
            "node": "Call 'PKM — Retrieval Config'",
            "type": "main"
          }
        ]
      ]
    },
    "If TEXT < 4000": {
      "main": [
        [
          {
            "index": 0,
            "node": "Create Whole Text Prompt",
            "type": "main"
          }
        ],
        [
          {
            "index": 0,
            "node": "Create Sampled Prompt",
            "type": "main"
          }
        ]
      ]
    },
    "Markdown Conversion": {
      "main": [
        [
          {
            "index": 0,
            "node": "Remove Boilerplate",
            "type": "main"
          }
        ]
      ]
    },
    "Merge": {
      "main": [
        [
          {
            "index": 0,
            "node": "Parse T1",
            "type": "main"
          }
        ]
      ]
    },
    "Message a model": {
      "main": [
        [
          {
            "index": 1,
            "node": "Merge",
            "type": "main"
          }
        ]
      ]
    },
    "Normalization1": {
      "main": [
        [
          {
            "index": 0,
            "node": "Thread Parser",
            "type": "main"
          }
        ]
      ]
    },
    "Normalize": {
      "main": [
        [
          {
            "index": 0,
            "node": "Markdown Conversion",
            "type": "main"
          }
        ]
      ]
    },
    "Parse T1": {
      "main": [
        [
          {
            "index": 0,
            "node": "Update T1 Info to DB",
            "type": "main"
          }
        ]
      ]
    },
    "Postgres INSERT": {
      "main": [
        [
          {
            "index": 0,
            "node": "Topics = {}",
            "type": "main"
          },
          {
            "index": 0,
            "node": "Merge",
            "type": "main"
          }
        ]
      ]
    },
    "Postgres Update Extract": {
      "main": [
        [
          {
            "index": 0,
            "node": "Compose Reply Text",
            "type": "main"
          }
        ]
      ]
    },
    "Remove Boilerplate": {
      "main": [
        [
          {
            "index": 0,
            "node": "Compute Retrieval Excerpt + Quality (email clean_text)",
            "type": "main"
          }
        ]
      ]
    },
    "Signatures and Markdown": {
      "main": [
        [
          {
            "index": 0,
            "node": "Compute Retrieval Excerpt + Quality (email clean_text)",
            "type": "main"
          }
        ]
      ]
    },
    "Switch": {
      "main": [
        [
          {
            "index": 0,
            "node": "Normalize",
            "type": "main"
          }
        ],
        [
          {
            "index": 0,
            "node": "Normalization1",
            "type": "main"
          }
        ]
      ]
    },
    "Thread Parser": {
      "main": [
        [
          {
            "index": 0,
            "node": "Signatures and Markdown",
            "type": "main"
          }
        ]
      ]
    },
    "Topics = {}": {
      "main": [
        [
          {
            "index": 0,
            "node": "If TEXT < 4000",
            "type": "main"
          }
        ]
      ]
    },
    "Update T1 Info to DB": {
      "main": [
        [
          {
            "index": 0,
            "node": "Postgres Update Extract",
            "type": "main"
          }
        ]
      ]
    }
  },
  "createdAt": "2026-01-24T05:20:03.503Z",
  "description": null,
  "isArchived": false,
  "name": "E-Mail Capture",
  "nodes": [
    {
      "id": "baf8de8d-5694-4d2f-9bef-295760ae0872",
      "name": "Switch",
      "parameters": {
        "options": {},
        "rules": {
          "values": [
            {
              "conditions": {
                "combinator": "and",
                "conditions": [
                  {
                    "id": "405552b3-f700-4ac0-96d3-b72570c8d02d",
                    "leftValue": "={{$json.content_type}}",
                    "operator": {
                      "operation": "equals",
                      "type": "string"
                    },
                    "rightValue": "newsletter"
                  }
                ],
                "options": {
                  "caseSensitive": true,
                  "leftValue": "",
                  "typeValidation": "strict",
                  "version": 3
                }
              },
              "outputKey": "newsletter",
              "renameOutput": true
            },
            {
              "conditions": {
                "combinator": "and",
                "conditions": [
                  {
                    "id": "d91ae66a-2dfb-4dda-8b31-69174e80e66b",
                    "leftValue": "={{$json.content_type}}",
                    "operator": {
                      "name": "filter.operator.equals",
                      "operation": "equals",
                      "type": "string"
                    },
                    "rightValue": "correspondence"
                  }
                ],
                "options": {
                  "caseSensitive": true,
                  "leftValue": "",
                  "typeValidation": "strict",
                  "version": 3
                }
              },
              "outputKey": "correspondence",
              "renameOutput": true
            }
          ]
        }
      },
      "position": [
        352,
        -480
      ],
      "type": "n8n-nodes-base.switch",
      "typeVersion": 3.4
    },
    {
      "id": "224e23d6-aa73-4cc8-917d-b6d951546451",
      "name": "Markdown Conversion",
      "notes": "## 3) Convert to Markdown\n- Turn paragraphs/lists/links into clean Markdown with stable formatting (no random headers).\n- Prevent artifacts from becoming headings (e.g., transport ids / internal anchors).\n- Preserve semantic breaks (paragraphs, list items), avoid collapsing links into one mega-line.\n- Output Markdown that is readable and diff-friendly (consistent line breaks, minimal noise).\n",
      "parameters": {
        "jsCode": "/**\n * Node2 — Markdown extraction (newsletter_text/newsletter_html -> newsletter_md)\n *\n * CONTRACT\n * Input:\n *   - newsletter_text (preferred plain text)\n *   - newsletter_html (optional HTML)\n *   - core_text (fallback if newsletter_text missing)\n *\n * Output:\n *   - newsletter_md: markdown-ish text suitable for final cleanup (Node3)\n *   - debug: which source was chosen and why (metrics + guardrails)\n *\n * PRINCIPLES\n *   - Always compute candidates:\n *       md_from_text\n *       md_from_html\n *   - Choose with quality gates (avoid prageng link-only HTML winning)\n *   - Cleanup is conservative: transport artifacts + whitespace only\n *   - Debug reflects the actual final selection + any overrides\n *\n * No external deps; n8n runner-safe.\n */\n\nconst nowIso = new Date().toISOString();\n\n/* ----------------------------- common helpers ----------------------------- */\n\nconst normalizeNewlines = (s) => String(s || \"\").replace(/\\r\\n/g, \"\\n\").replace(/\\r/g, \"\\n\");\n\nconst decodeEntities = (s) =>\n  String(s || \"\")\n    .replace(/&nbsp;/gi, \" \")\n    .replace(/&#160;/g, \" \")\n    .replace(/&amp;/g, \"&\")\n    .replace(/&lt;/g, \"<\")\n    .replace(/&gt;/g, \">\")\n    .replace(/&quot;/g, '\"')\n    .replace(/&#39;/g, \"'\")\n    .replace(/&apos;/g, \"'\");\n\nconst makeCfRegex = () => {\n  try {\n    return new RegExp(\"\\\\p{Cf}+\", \"gu\");\n  } catch {\n    return /[\\u200B-\\u200F\\u202A-\\u202E\\u2060-\\u206F\\uFEFF\\u00AD\\u034F]/g;\n  }\n};\nconst RE_CF = makeCfRegex();\n\nconst RE_UNI_SPACE = /[\\u00A0\\u1680\\u2000-\\u200A\\u2007\\u202F\\u205F\\u3000]/g;\n\nfunction stripInvisibleTransport(s) {\n  return String(s || \"\")\n    .replace(/\\u0000/g, \"\")\n    .replace(RE_UNI_SPACE, \" \")\n    .replace(RE_CF, \"\")\n    .replace(/\\u00A0/g, \" \");\n}\n\nfunction collapseWhitespacePreserveNewlines(s) {\n  const text = normalizeNewlines(s);\n  const lines = text.split(\"\\n\");\n\n  const out = [];\n  let blankRun = 0;\n\n  for (let line of lines) {\n    line = String(line || \"\")\n      .replace(/[ \\t]+/g, \" \")\n      .trim();\n\n    if (!line) {\n      blankRun += 1;\n      if (blankRun <= 2) out.push(\"\");\n      continue;\n    }\n\n    blankRun = 0;\n    out.push(line);\n  }\n\n  return out.join(\"\\n\").trim();\n}\n\nfunction dropTransportArtifactLines(text) {\n  const lines = normalizeNewlines(text).split(\"\\n\");\n  const out = [];\n  for (const line of lines) {\n    const t = String(line || \"\").trim();\n    if (/^<#m_-?\\d+_>\\s*$/i.test(t)) continue;\n    out.push(line);\n  }\n  return out.join(\"\\n\");\n}\n\n/* ----------------------------- mojibake guard ---------------------------- */\n\nconst MOJI_SIGNAL = /(?:â[\\u0080-\\u00BF]|Ã[\\u0080-\\u00BF]|Â[\\u0080-\\u00BF]|â€”|â€“|â€™|â€œ|â€\\x9d|â€¢|â€¦|Â )/;\n\nfunction mojiScore(s) {\n  const str = String(s || \"\");\n  if (!str) return 0;\n  const m = str.match(new RegExp(MOJI_SIGNAL.source, \"g\"));\n  const scoreSignals = m ? m.length : 0;\n  const scoreRepl = (str.match(/\\uFFFD/g) || []).length;\n  return scoreSignals + scoreRepl;\n}\n\nfunction fixMojibakeGuarded(s) {\n  const str = String(s || \"\");\n  if (!str) return { text: str, fixed: false, method: \"none\" };\n\n  if (!MOJI_SIGNAL.test(str) && str.indexOf(\"\\uFFFD\") === -1) {\n    return { text: str, fixed: false, method: \"none\" };\n  }\n\n  try {\n    const bytes = Uint8Array.from(str, (c) => c.charCodeAt(0) & 0xff);\n\n    let out = null;\n    if (typeof TextDecoder !== \"undefined\") {\n      out = new TextDecoder(\"utf-8\", { fatal: false }).decode(bytes);\n    } else if (typeof Buffer !== \"undefined\") {\n      out = Buffer.from(bytes).toString(\"utf8\");\n    }\n\n    if (out && out !== str && mojiScore(out) <= mojiScore(str)) {\n      return { text: out, fixed: true, method: \"latin1->utf8\" };\n    }\n  } catch (_) {}\n\n  const out2 = str\n    .replace(/â€™/g, \"’\")\n    .replace(/â€œ/g, \"“\")\n    .replace(/â€\\x9d/g, \"”\")\n    .replace(/â€“/g, \"–\")\n    .replace(/â€”/g, \"—\")\n    .replace(/â€¦/g, \"…\")\n    .replace(/â€¢/g, \"•\")\n    .replace(/Â /g, \" \")\n    .replace(/Â/g, \"\");\n\n  return { text: out2, fixed: out2 !== str, method: out2 !== str ? \"replace\" : \"none\" };\n}\n\n/* ---------------------------- HTML -> Markdown ---------------------------- */\n/**\n * Best-effort, regex-based converter. IMPORTANT: does NOT delete containers by keywords.\n * Keeps content; only strips scripts/styles and transforms tags into readable text.\n */\n\nfunction htmlToMarkdownSimple(html) {\n  if (!html) return { md: \"\", debug: { steps: [] } };\n\n  let h = String(html);\n\n  // hard cap (defensive)\n  if (h.length > 2_000_000) h = h.slice(0, 2_000_000);\n\n  // normalize newlines early\n  h = normalizeNewlines(h);\n\n  // remove scripts/styles/comments\n  h = h.replace(/<script[\\s\\S]*?<\\/script>/gi, \"\");\n  h = h.replace(/<style[\\s\\S]*?<\\/style>/gi, \"\");\n  h = h.replace(/<!--([\\s\\S]*?)-->/g, \"\");\n\n  // drop tracking pixel imgs (1x1 or hidden)\n  h = h.replace(\n    /<img[^>]+(?:width=[\"']?\\s*(?:0|1)\\s*(?:px)?[\"']?|height=[\"']?\\s*(?:0|1)\\s*(?:px)?[\"']?|style=[\"'][^\"']*(?:display\\s*:\\s*none|opacity\\s*:\\s*0|visibility\\s*:\\s*hidden|width\\s*:\\s*(?:0|1)px|height\\s*:\\s*(?:0|1)px)[^\"']*[\"'])[^>]*>/gi,\n    \"\"\n  );\n\n  // convert <br> to newline\n  h = h.replace(/<br\\s*\\/?>/gi, \"\\n\");\n\n  // headings -> markdown headings\n  for (let k = 1; k <= 6; k++) {\n    const re = new RegExp(`<h${k}[^>]*>([\\\\s\\\\S]*?)<\\\\/h${k}>`, \"gi\");\n    const hashes = \"#\".repeat(k);\n    h = h.replace(re, (_, inner) => `\\n${hashes} ${inner}\\n`);\n  }\n\n  // list items -> \"- \"\n  h = h.replace(/<li[^>]*>/gi, \"\\n- \");\n  h = h.replace(/<\\/li>/gi, \"\");\n\n  // block-ish tags -> paragraph breaks\n  h = h.replace(/<\\/(p|div|table|tr|td|section|article|header|footer|blockquote)>/gi, \"\\n\\n\");\n  h = h.replace(/<(p|div|table|tr|td|section|article|header|footer|blockquote)[^>]*>/gi, \"\\n\");\n\n  // <hr> -> separator\n  h = h.replace(/<hr[^>]*>/gi, \"\\n---\\n\");\n\n  // anchors: <a href=\"...\">text</a> -> [text](url)\n  // Keep it conservative: if text is empty, use url as text; if url missing, keep text.\n  h = h.replace(/<a\\s+[^>]*href=[\"']([^\"']+)[\"'][^>]*>([\\s\\S]*?)<\\/a>/gi, (_, href, inner) => {\n    const url = String(href || \"\").trim();\n    const txt = String(inner || \"\").replace(/<[^>]+>/g, \" \").replace(/\\s+/g, \" \").trim();\n    if (!url) return txt || \"\";\n    const label = txt || url;\n    return `[${label}](${url})`;\n  });\n\n  // emphasis/bold (simple)\n  h = h.replace(/<(strong|b)[^>]*>([\\s\\S]*?)<\\/\\1>/gi, (_, __, inner) => `**${inner}**`);\n  h = h.replace(/<(em|i)[^>]*>([\\s\\S]*?)<\\/\\1>/gi, (_, __, inner) => `*${inner}*`);\n\n  // images: prefer alt text; if no alt, ignore\n  h = h.replace(/<img[^>]*alt=[\"']([^\"']+)[\"'][^>]*>/gi, (_, alt) => {\n    const a = String(alt || \"\").trim();\n    return a ? `\\n![${a}]\\n` : \"\";\n  });\n  h = h.replace(/<img[^>]*>/gi, \"\");\n\n  // strip remaining tags\n  h = h.replace(/<[^>]+>/g, \" \");\n\n  // entity decode + transport cleanup\n  h = decodeEntities(h);\n  h = stripInvisibleTransport(h);\n\n  // normalize whitespace\n  h = h.replace(/[ \\t]+/g, \" \");\n  h = collapseWhitespacePreserveNewlines(h);\n\n  // reduce markdown noise: collapse repeated separators\n  h = h.replace(/\\n(?:---\\n){2,}/g, \"\\n---\\n\");\n\n  return { md: h.trim() };\n}\n\n/* ---------------------------- candidate building ---------------------------- */\n\nfunction buildMdFromText(textInput) {\n  let t = String(textInput || \"\");\n  const moji = fixMojibakeGuarded(t);\n  t = moji.text;\n\n  t = decodeEntities(t);\n  t = stripInvisibleTransport(t);\n  t = dropTransportArtifactLines(t);\n  t = collapseWhitespacePreserveNewlines(t);\n\n  // Minimal readability tweaks (NOT semantic deletion):\n  // - Ensure there's a blank line before obvious headings (ALL CAPS line or \"Title:\" pattern)\n  const lines = t.split(\"\\n\");\n  const out = [];\n  for (let i = 0; i < lines.length; i++) {\n    const line = lines[i];\n    const prev = out.length ? out[out.length - 1] : \"\";\n\n    const trimmed = String(line || \"\").trim();\n    const isAllCaps =\n      trimmed.length >= 6 &&\n      trimmed.length <= 80 &&\n      /^[A-Z0-9][A-Z0-9\\s\\-:&/]+$/.test(trimmed) &&\n      (trimmed.match(/[A-Z]/g) || []).length >= 4;\n\n    const looksHeading = isAllCaps || /^#{1,6}\\s+/.test(trimmed);\n\n    if (looksHeading && prev && prev.trim() !== \"\") out.push(\"\");\n    out.push(line);\n  }\n\n  t = collapseWhitespacePreserveNewlines(out.join(\"\\n\"));\n\n  return { md: t, mojibakeFixed: !!moji.fixed, mojibakeMethod: moji.method };\n}\n\n/* ---------------------------- scoring / chooser ---------------------------- */\n\nconst URL_RE = /(https?:\\/\\/[^\\s<>()]+|www\\.[^\\s<>()]+)/ig;\n\nfunction countLetters(s) {\n  const str = String(s || \"\");\n  try {\n    const m = str.match(/\\p{L}/gu);\n    return m ? m.length : 0;\n  } catch {\n    const m = str.match(/[A-Za-z]/g);\n    return m ? m.length : 0;\n  }\n}\n\nfunction computeMetrics(md) {\n  const s = String(md || \"\");\n  const len = s.length;\n\n  const letters = countLetters(s);\n  const alphaRatio = len ? letters / len : 0;\n\n  const urls = s.match(URL_RE) || [];\n  const urlCount = urls.length;\n  const urlChars = urls.reduce((a, u) => a + u.length, 0);\n  const urlCharDensity = len ? urlChars / len : 0;\n\n  const lines = normalizeNewlines(s).split(\"\\n\");\n  const nonEmpty = lines.filter((l) => String(l || \"\").trim() !== \"\");\n  const lineCount = nonEmpty.length || 0;\n\n  let urlOnlyLines = 0;\n  for (const line of nonEmpty) {\n    const t = String(line || \"\").trim();\n    const noUrls = t.replace(URL_RE, \"\").replace(/[\\s\\W_]+/g, \"\");\n    const hasUrl = URL_RE.test(t);\n    // reset regex state (global)\n    URL_RE.lastIndex = 0;\n\n    if (hasUrl && noUrls.length === 0) urlOnlyLines += 1;\n  }\n\n  const urlOnlyPct = lineCount ? urlOnlyLines / lineCount : 0;\n\n  return {\n    len,\n    letters,\n    alphaRatio,\n    urlCount,\n    urlChars,\n    urlCharDensity,\n    lineCount,\n    urlOnlyLines,\n    urlOnlyPct,\n  };\n}\n\nfunction scoreCandidate(m) {\n  // higher is better\n  // emphasize textiness, penalize url dominance, small boost for “not tiny”\n  const lenBoost = Math.min(1, m.len / 2500) * 10; // max +10\n  const s =\n    m.alphaRatio * 100 +\n    lenBoost -\n    m.urlCharDensity * 70 -\n    m.urlOnlyPct * 60 -\n    Math.min(30, Math.log10(1 + m.urlCount) * 8); // mild penalty for many urls\n  return s;\n}\n\nfunction classifyBadHtml(mdMetrics) {\n  // “mostly URLs” / “low textiness” hard gates to fix prageng-style failures\n  const bad =\n    mdMetrics.len > 0 &&\n    (mdMetrics.urlOnlyPct >= 0.45 ||\n      mdMetrics.alphaRatio <= 0.085 ||\n      (mdMetrics.urlCharDensity >= 0.55 && mdMetrics.alphaRatio < 0.16));\n\n  const reasons = [];\n  if (mdMetrics.urlOnlyPct >= 0.45) reasons.push(\"html_url_only_lines_high\");\n  if (mdMetrics.alphaRatio <= 0.085) reasons.push(\"html_alpha_density_low\");\n  if (mdMetrics.urlCharDensity >= 0.55 && mdMetrics.alphaRatio < 0.16) reasons.push(\"html_url_density_high\");\n\n  return { bad, reasons };\n}\n\nfunction pickBestCandidate(textCand, htmlCand, coreFallbackCand) {\n  // Compute metrics + scores\n  const mt = computeMetrics(textCand.md);\n  const mh = computeMetrics(htmlCand.md);\n  const mc = computeMetrics(coreFallbackCand.md);\n\n  const st = scoreCandidate(mt);\n  const sh = scoreCandidate(mh);\n  const sc = scoreCandidate(mc);\n\n  const reasons = [];\n  let used = \"text\";\n  let chosen = textCand.md;\n  let chosenMetrics = mt;\n  let chosenScore = st;\n\n  // Decide text candidate input source label (newsletter_text vs core_text)\n  const textIsEmpty = mt.len === 0;\n  const coreHasContent = mc.len > 0;\n\n  if (textIsEmpty && coreHasContent) {\n    used = \"core_fallback\";\n    chosen = coreFallbackCand.md;\n    chosenMetrics = mc;\n    chosenScore = sc;\n    reasons.push(\"newsletter_text_empty_used_core_fallback\");\n  }\n\n  // If HTML is available, evaluate it vs the currently chosen text-ish candidate\n  const htmlExists = mh.len > 0;\n\n  if (htmlExists) {\n    const badHtml = classifyBadHtml(mh);\n\n    if (badHtml.bad) {\n      reasons.push(...badHtml.reasons);\n      reasons.push(\"prefer_text_due_to_html_quality_gate\");\n    } else {\n      // Compare scores; also consider length ratio\n      const otherMetrics = chosenMetrics;\n      const otherScore = chosenScore;\n\n      const lenRatio = otherMetrics.len > 0 ? mh.len / otherMetrics.len : 0;\n\n      // If HTML is dramatically shorter, be skeptical unless score is MUCH higher\n      const htmlMuchShorter = otherMetrics.len >= 800 && mh.len < otherMetrics.len * 0.35;\n\n      if (sh > otherScore + (htmlMuchShorter ? 12 : 3)) {\n        used = \"html\";\n        chosen = htmlCand.md;\n        chosenMetrics = mh;\n        chosenScore = sh;\n        reasons.push(\"html_score_higher\");\n        if (htmlMuchShorter) reasons.push(\"html_short_but_score_strong\");\n      } else {\n        reasons.push(\"text_score_higher_or_html_not_better\");\n      }\n\n      // If HTML is slightly better but mostly duplicative/linky, keep text.\n      if (used === \"html\" && (mh.urlOnlyPct > otherMetrics.urlOnlyPct + 0.15)) {\n        used = used === \"core_fallback\" ? \"core_fallback\" : \"text\";\n        chosen = used === \"core_fallback\" ? coreFallbackCand.md : textCand.md;\n        chosenMetrics = used === \"core_fallback\" ? mc : mt;\n        chosenScore = used === \"core_fallback\" ? sc : st;\n        reasons.push(\"override_html_due_to_higher_url_only_pct\");\n      }\n\n      // Guardrail: if chosen ends up extremely small relative to the other, revert\n      const altMd = used === \"html\" ? (chosenMetrics === mh ? (textIsEmpty ? coreFallbackCand.md : textCand.md) : \"\") : htmlCand.md;\n      const altMetrics = used === \"html\" ? (textIsEmpty ? mc : mt) : mh;\n\n      if (\n        chosenMetrics.len > 0 &&\n        altMetrics.len >= 900 &&\n        chosenMetrics.len < altMetrics.len * 0.10\n      ) {\n        // revert to the larger candidate\n        if (used === \"html\") {\n          used = textIsEmpty ? \"core_fallback\" : \"text\";\n          chosen = textIsEmpty ? coreFallbackCand.md : textCand.md;\n          chosenMetrics = textIsEmpty ? mc : mt;\n          chosenScore = textIsEmpty ? sc : st;\n          reasons.push(\"guardrail_revert_html_too_small\");\n        } else {\n          // revert to html only if it was the larger one (rare here)\n          used = \"html\";\n          chosen = htmlCand.md;\n          chosenMetrics = mh;\n          chosenScore = sh;\n          reasons.push(\"guardrail_revert_text_too_small\");\n        }\n      }\n    }\n  } else {\n    reasons.push(\"no_html_candidate\");\n  }\n\n  return {\n    used,\n    reason: reasons.join(\"; \"),\n    chosen,\n    chosenMetrics,\n    chosenScore,\n    mt,\n    mh,\n    mc,\n    st,\n    sh,\n    sc,\n  };\n}\n\n/* ---------------------------------- inputs ---------------------------------- */\n\nconst newsletterTextIn = String($json.newsletter_text || \"\");\nconst newsletterHtmlIn = $json.newsletter_html || null;\nconst coreTextIn = String($json.core_text || \"\");\n\n/* ---------------------------- build candidates ---------------------------- */\n\nconst textCand = buildMdFromText(newsletterTextIn);\nconst coreCand = buildMdFromText(coreTextIn);\n\nlet htmlCand = { md: \"\" };\nlet htmlMojibakeFixed = false;\nlet htmlMojibakeMethod = \"none\";\nif (newsletterHtmlIn && String(newsletterHtmlIn).trim()) {\n  const moji = fixMojibakeGuarded(String(newsletterHtmlIn));\n  const conv = htmlToMarkdownSimple(moji.text);\n  htmlCand = { md: conv.md };\n\n  htmlMojibakeFixed = !!moji.fixed;\n  htmlMojibakeMethod = moji.method;\n}\n\n/* ------------------------------ choose best ------------------------------ */\n\nconst choice = pickBestCandidate(textCand, htmlCand, coreCand);\n\n// Conservative final cleanup only (no semantic deletions)\nlet newsletter_md = choice.chosen;\nnewsletter_md = dropTransportArtifactLines(newsletter_md);\nnewsletter_md = collapseWhitespacePreserveNewlines(newsletter_md);\n\n/* ---------------------------------- debug ---------------------------------- */\n\nfunction round3(x) {\n  return Math.round((x + Number.EPSILON) * 1000) / 1000;\n}\n\nreturn [\n  {\n    json: {\n      ...$json,\n\n      newsletter_md,\n      node2_at: nowIso,\n\n      // Actual branch used (set at selection time)\n      debug_used: choice.used,\n      debug_choice_reason: choice.reason,\n      debug_choice_score: round3(choice.chosenScore),\n\n      // Final output metrics\n      debug_md_len: choice.chosenMetrics.len,\n      debug_alpha_ratio: round3(choice.chosenMetrics.alphaRatio),\n      debug_url_count: choice.chosenMetrics.urlCount,\n      debug_url_char_density: round3(choice.chosenMetrics.urlCharDensity),\n      debug_url_lines_pct: round3(choice.chosenMetrics.urlOnlyPct),\n\n      // Candidate metrics (for audits / regression)\n      debug_text_md_len: choice.mt.len,\n      debug_text_alpha_ratio: round3(choice.mt.alphaRatio),\n      debug_text_url_lines_pct: round3(choice.mt.urlOnlyPct),\n      debug_text_url_char_density: round3(choice.mt.urlCharDensity),\n      debug_text_score: round3(choice.st),\n      debug_text_mojibake_fixed: !!textCand.mojibakeFixed,\n      debug_text_mojibake_method: textCand.mojibakeMethod,\n\n      debug_html_md_len: choice.mh.len,\n      debug_html_alpha_ratio: round3(choice.mh.alphaRatio),\n      debug_html_url_lines_pct: round3(choice.mh.urlOnlyPct),\n      debug_html_url_char_density: round3(choice.mh.urlCharDensity),\n      debug_html_score: round3(choice.sh),\n      debug_html_input_mojibake_fixed: htmlMojibakeFixed,\n      debug_html_input_mojibake_method: htmlMojibakeMethod,\n\n      debug_core_fallback_md_len: choice.mc.len,\n      debug_core_fallback_alpha_ratio: round3(choice.mc.alphaRatio),\n      debug_core_fallback_url_lines_pct: round3(choice.mc.urlOnlyPct),\n      debug_core_fallback_url_char_density: round3(choice.mc.urlCharDensity),\n      debug_core_fallback_score: round3(choice.sc),\n    },\n  },\n];\n"
      },
      "position": [
        800,
        -576
      ],
      "type": "n8n-nodes-base.code",
      "typeVersion": 2
    },
    {
      "id": "02e08da0-cb80-465c-b77b-d881bfe5a0b0",
      "name": "Remove Boilerplate",
      "notes": "## 4) Clean Boilerplate\n- Detect and remove footers/CTAs/nav: unsubscribe, preferences, archives, privacy, address blocks, “email to a friend”.\n- Remove tracking/link-dumps and high link-density spans; keep at most one canonical “view online” link (optional).\n- Deduplicate teaser vs full-body by selecting the strongest main-content block (length + low link density).\n- Output final “clean_md / clean_text” + optional separated metadata (title, canonical_url, tags).",
      "parameters": {
        "jsCode": "/**\n * Node3 — Boilerplate cleanup & polish (newsletter_md -> clean_text + metadata)\n *\n * CONTRACT\n * Input:\n *   - newsletter_md (+ existing metadata fields like author/url_canonical/title/etc)\n *\n * Output:\n *   - clean_text: readable final content (must not be empty unless input is empty)\n *   - url_canonical: best “view online” URL if available (do not overwrite non-empty)\n *   - author: may be derived conservatively, but must NOT override non-empty author\n *   - debug fields: what was removed, anchor decisions, safety reverts\n *\n * PRINCIPLES\n *   - Teaser deletion belongs upstream (Node1). Node3 only does it with VERY strong evidence,\n *     and only if Node1 did NOT already drop teaser (debug_teaser_dropped !== true).\n *   - Footer trimming is anchor-based (handles “address last”).\n *   - Never output empty unless input is effectively empty (hard requirement).\n *   - View-online removal is strict (label line only + URL captured).\n *   - FeedBlitz tracking removal only affects URL-only lines on known tracking hosts,\n *     or surgically removes the URL when line has real text.\n *\n * No external deps; n8n runner-safe.\n */\n\nconst nowIso = new Date().toISOString();\n\n/* ----------------------------- common helpers ----------------------------- */\n\nconst normalizeNewlines = (s) => String(s || \"\").replace(/\\r\\n/g, \"\\n\").replace(/\\r/g, \"\\n\");\n\nconst decodeEntities = (s) =>\n  String(s || \"\")\n    .replace(/&nbsp;/gi, \" \")\n    .replace(/&#160;/g, \" \")\n    .replace(/&amp;/g, \"&\")\n    .replace(/&lt;/g, \"<\")\n    .replace(/&gt;/g, \">\")\n    .replace(/&quot;/g, '\"')\n    .replace(/&#39;/g, \"'\")\n    .replace(/&apos;/g, \"'\");\n\nconst makeCfRegex = () => {\n  try {\n    return new RegExp(\"\\\\p{Cf}+\", \"gu\");\n  } catch {\n    return /[\\u200B-\\u200F\\u202A-\\u202E\\u2060-\\u206F\\uFEFF\\u00AD\\u034F]/g;\n  }\n};\nconst RE_CF = makeCfRegex();\nconst RE_UNI_SPACE = /[\\u00A0\\u1680\\u2000-\\u200A\\u2007\\u202F\\u205F\\u3000]/g;\n\nfunction stripInvisibleTransport(s) {\n  return String(s || \"\")\n    .replace(/\\u0000/g, \"\")\n    .replace(RE_UNI_SPACE, \" \")\n    .replace(RE_CF, \"\")\n    .replace(/\\u00A0/g, \" \");\n}\n\nfunction collapseWhitespacePreserveNewlines(s) {\n  const text = normalizeNewlines(s);\n  const lines = text.split(\"\\n\");\n\n  const out = [];\n  let blankRun = 0;\n\n  for (let line of lines) {\n    line = String(line || \"\")\n      .replace(/[ \\t]+/g, \" \")\n      .trim();\n\n    if (!line) {\n      blankRun += 1;\n      if (blankRun <= 2) out.push(\"\");\n      continue;\n    }\n\n    blankRun = 0;\n    out.push(line);\n  }\n\n  return out.join(\"\\n\").trim();\n}\n\n/* ----------------------------- mojibake guard ---------------------------- */\n\nconst MOJI_SIGNAL = /(?:â[\\u0080-\\u00BF]|Ã[\\u0080-\\u00BF]|Â[\\u0080-\\u00BF]|â€”|â€“|â€™|â€œ|â€\\x9d|â€¢|â€¦|Â )/;\n\nfunction mojiScore(s) {\n  const str = String(s || \"\");\n  if (!str) return 0;\n  const m = str.match(new RegExp(MOJI_SIGNAL.source, \"g\"));\n  const scoreSignals = m ? m.length : 0;\n  const scoreRepl = (str.match(/\\uFFFD/g) || []).length;\n  return scoreSignals + scoreRepl;\n}\n\nfunction fixMojibakeGuarded(s) {\n  const str = String(s || \"\");\n  if (!str) return { text: str, fixed: false, method: \"none\" };\n\n  if (!MOJI_SIGNAL.test(str) && str.indexOf(\"\\uFFFD\") === -1) {\n    return { text: str, fixed: false, method: \"none\" };\n  }\n\n  try {\n    const bytes = Uint8Array.from(str, (c) => c.charCodeAt(0) & 0xff);\n\n    let out = null;\n    if (typeof TextDecoder !== \"undefined\") {\n      out = new TextDecoder(\"utf-8\", { fatal: false }).decode(bytes);\n    } else if (typeof Buffer !== \"undefined\") {\n      out = Buffer.from(bytes).toString(\"utf8\");\n    }\n\n    if (out && out !== str && mojiScore(out) <= mojiScore(str)) {\n      return { text: out, fixed: true, method: \"latin1->utf8\" };\n    }\n  } catch (_) {}\n\n  const out2 = str\n    .replace(/â€™/g, \"’\")\n    .replace(/â€œ/g, \"“\")\n    .replace(/â€\\x9d/g, \"”\")\n    .replace(/â€“/g, \"–\")\n    .replace(/â€”/g, \"—\")\n    .replace(/â€¦/g, \"…\")\n    .replace(/â€¢/g, \"•\")\n    .replace(/Â /g, \" \")\n    .replace(/Â/g, \"\");\n\n  return { text: out2, fixed: out2 !== str, method: out2 !== str ? \"replace\" : \"none\" };\n}\n\n/* ------------------------------- URL helpers ------------------------------ */\n\nconst URL_RE = /(https?:\\/\\/[^\\s)<>]+|www\\.[^\\s)<>]+)/ig;\n\nfunction extractUrls(line) {\n  const m = String(line || \"\").match(URL_RE);\n  return m ? m.map((u) => u.trim()) : [];\n}\n\nfunction firstUrl(line) {\n  const u = extractUrls(line);\n  return u.length ? u[0] : null;\n}\n\nfunction urlFromMarkdownLink(line) {\n  const m = String(line || \"\").match(/\\[[^\\]]*\\]\\((https?:\\/\\/[^)]+)\\)/i);\n  return m ? m[1] : null;\n}\n\nfunction parseHost(url) {\n  const u = String(url || \"\").trim();\n  if (!u) return null;\n  try {\n    const fixed = u.startsWith(\"http\") ? u : `https://${u}`;\n    return new URL(fixed).hostname.toLowerCase();\n  } catch {\n    return null;\n  }\n}\n\nfunction lineIsUrlOnly(line) {\n  const l = String(line || \"\").trim();\n  if (!l) return false;\n\n  const urls = extractUrls(l);\n  if (!urls.length) return false;\n\n  // remove markdown links first: \"[x](url)\" -> \"\"\n  let t = l.replace(/\\[[^\\]]*\\]\\((https?:\\/\\/[^)]+)\\)/gi, \"\");\n\n  // remove bare urls\n  t = t.replace(URL_RE, \"\");\n\n  // remove angle brackets + punctuation + whitespace\n  t = t.replace(/[<>\\[\\]()*•·—–\\-_,.:;'\"“”‘’|/\\\\]/g, \"\");\n  t = t.replace(/\\s+/g, \"\").trim();\n\n  return t.length === 0;\n}\n\nfunction countLetters(s) {\n  const str = String(s || \"\");\n  try {\n    const m = str.match(/\\p{L}/gu);\n    return m ? m.length : 0;\n  } catch {\n    const m = str.match(/[A-Za-z]/g);\n    return m ? m.length : 0;\n  }\n}\n\n/* ----------------------- transport / artifact filters --------------------- */\n\nfunction isTransportArtifactLine(line) {\n  const l = String(line || \"\").trim();\n  return /^<?#?m_[\\w-]+_>?$/i.test(l) || /^<#m_-?\\d+_>\\s*$/i.test(l);\n}\n\nfunction isEmptyAngleLink(line) {\n  const l = String(line || \"\").trim();\n  return /^<\\s*>$/.test(l);\n}\n\nfunction isDecorativeSeparator(line) {\n  const l = String(line || \"\").trim();\n  if (!l) return false;\n  // keep markdown \"---\" (meaningful), but remove long dash runs\n  if (l === \"---\") return false;\n  return /^-{12,}$/.test(l) || /^_{12,}$/.test(l) || /^={12,}$/.test(l);\n}\n\n/* -------------------------- view online (strict) -------------------------- */\n\nfunction looksViewOnlineLabelLineStrict(line) {\n  const raw = String(line || \"\").trim();\n  if (!raw) return false;\n\n  // exact labels\n  const l = raw.toLowerCase();\n\n  // a markdown link that is ONLY \"view online\"/\"view in browser\"\n  const md = raw.match(/^\\s*\\[([^\\]]+)\\]\\((https?:\\/\\/[^)]+)\\)\\s*$/i);\n  if (md) {\n    const label = String(md[1] || \"\").trim().toLowerCase();\n    return (\n      label === \"view online\" ||\n      label === \"view in browser\" ||\n      label === \"view in your browser\" ||\n      label === \"open in browser\" ||\n      label === \"read online\" ||\n      label === \"web version\"\n    );\n  }\n\n  // strict short label line (not prose)\n  if (raw.length > 42) return false;\n\n  return /^(view (online|in (your )?browser)|open in browser|read online|web version)\\s*[:\\-–—]?\\s*$/i.test(\n    raw\n  );\n}\n\nfunction tryExtractViewOnline(lines) {\n  // scan first 30 lines (including blanks, but label is usually non-empty)\n  const MAX = Math.min(30, lines.length);\n\n  for (let i = 0; i < MAX; i++) {\n    const cur = String(lines[i] || \"\").trim();\n    if (!cur) continue;\n\n    if (!looksViewOnlineLabelLineStrict(cur)) continue;\n\n    // capture url from same line or next non-empty line (ideally url-only)\n    let url =\n      urlFromMarkdownLink(cur) ||\n      firstUrl(cur) ||\n      urlFromMarkdownLink(lines[i + 1] || \"\") ||\n      firstUrl(lines[i + 1] || \"\");\n\n    if (!url) {\n      // cannot safely remove anything\n      return { found: true, removed: false, url: null, at: i, reason: \"label_found_no_url\" };\n    }\n\n    // remove label line, and remove next line only if it's url-only\n    lines[i] = \"\";\n\n    const next = String(lines[i + 1] || \"\").trim();\n    if (next && lineIsUrlOnly(next)) {\n      lines[i + 1] = \"\";\n    }\n\n    return { found: true, removed: true, url, at: i, reason: \"label_and_url_removed\" };\n  }\n\n  return { found: false, removed: false, url: null, at: -1, reason: \"not_found\" };\n}\n\n/* ------------------------ FeedBlitz tracking removal ----------------------- */\n\nfunction isFeedblitzTrackingHost(host) {\n  if (!host) return false;\n  return (\n    host === \"p.feedblitz.com\" ||\n    host === \"app.feedblitz.com\" ||\n    host === \"archive.feedblitz.com\" ||\n    host === \"feeds.feedblitz.com\"\n  );\n}\n\nfunction removeFeedblitzTracking(lines) {\n  let removedLines = 0;\n  let strippedUrls = 0;\n\n  const out = [];\n\n  for (let line of lines) {\n    const raw = String(line || \"\");\n    const trimmed = raw.trim();\n    if (!trimmed) {\n      out.push(raw);\n      continue;\n    }\n\n    const urls = extractUrls(trimmed);\n    if (!urls.length) {\n      out.push(raw);\n      continue;\n    }\n\n    const hosts = urls.map(parseHost);\n    const hasFeedblitz = hosts.some(isFeedblitzTrackingHost);\n\n    if (!hasFeedblitz) {\n      out.push(raw);\n      continue;\n    }\n\n    // If the whole line is just a tracking URL (or multiple URLs), remove it.\n    if (lineIsUrlOnly(trimmed) && hosts.every((h) => isFeedblitzTrackingHost(h))) {\n      removedLines++;\n      continue;\n    }\n\n    // Otherwise, surgically remove only feedblitz URLs, keep the text.\n    let newLine = raw;\n    for (const u of urls) {\n      const h = parseHost(u);\n      if (isFeedblitzTrackingHost(h)) {\n        // remove the exact URL substring; do not try to be smart about punctuation beyond whitespace cleanup\n        newLine = newLine.split(u).join(\"\");\n        strippedUrls++;\n      }\n    }\n\n    // cleanup if we stripped something\n    newLine = newLine.replace(/\\(\\s*\\)/g, \" \");\n    newLine = newLine.replace(/[ \\t]+/g, \" \").trim();\n\n    // If line becomes empty or effectively url-only after stripping, drop it.\n    if (!newLine || newLine.trim() === \"\" || lineIsUrlOnly(newLine)) {\n      removedLines++;\n      continue;\n    }\n\n    out.push(newLine);\n  }\n\n  return { lines: out, removedLines, strippedUrls };\n}\n\n/* ------------------------- teaser removal (very strong) ------------------------- */\n/**\n * Preferred behavior: do nothing (Node1 owns teaser removal).\n * If Node1 didn't run / didn't drop, allow a very-strong-evidence drop:\n *   - only around first long dashed separator (or markdown --- separator)\n *   - after is MUCH longer\n *   - before is shortish AND teaserish OR duplicated in after strongly\n * Always record removed span in debug_teaser_removed.\n */\n\nfunction normalizeForDup(s) {\n  return String(s || \"\")\n    .toLowerCase()\n    .replace(URL_RE, \" \")\n    .replace(/[^a-z0-9]+/g, \" \")\n    .replace(/\\s+/g, \" \")\n    .trim();\n}\n\nfunction maybeDropTeaserVeryStrong(text) {\n  const t = normalizeNewlines(String(text || \"\")).trim();\n  if (!t) return { text: t, dropped: false, removed: \"\", reason: \"empty\" };\n\n  const lines = t.split(\"\\n\");\n\n  // find first separator line: either a big dash run OR markdown \"---\" line\n  let sep = -1;\n  for (let i = 0; i < Math.min(lines.length, 220); i++) {\n    const l = String(lines[i] || \"\").trim();\n    if (l === \"---\" || /^-{20,}$/.test(l)) {\n      sep = i;\n      break;\n    }\n  }\n  if (sep === -1) return { text: t, dropped: false, removed: \"\", reason: \"no_separator\" };\n\n  const before = lines.slice(0, sep).join(\"\\n\").trim();\n  const after = lines.slice(sep + 1).join(\"\\n\").trim();\n\n  if (!before || !after) return { text: t, dropped: false, removed: \"\", reason: \"no_before_or_after\" };\n\n  // VERY conservative thresholds\n  if (!(after.length >= before.length * 2.2 && after.length >= 1000)) {\n    return { text: t, dropped: false, removed: \"\", reason: \"after_not_much_longer\" };\n  }\n  if (before.length > 1200) {\n    return { text: t, dropped: false, removed: \"\", reason: \"before_too_large\" };\n  }\n\n  const teaserish = /(\\.\\.\\.\\s*$|…\\s*$)/m.test(before) || /view online|view in (your )?browser/i.test(before);\n\n  const bN = normalizeForDup(before);\n  const aN = normalizeForDup(after);\n\n  const needle = bN.slice(0, Math.min(500, bN.length));\n  const duplicated = needle.length >= 160 && aN.includes(needle);\n\n  if (!(teaserish || duplicated)) {\n    return { text: t, dropped: false, removed: \"\", reason: \"no_strong_teaser_signal\" };\n  }\n\n  return {\n    text: after.trim(),\n    dropped: true,\n    removed: before.trim(),\n    reason: duplicated ? \"dup_substring_strong\" : \"teaserish_strong\",\n  };\n}\n\n/* --------------------------- footer trimming (anchor) --------------------------- */\n\nconst FOOTER_ANCHORS_STRONG = [\n  /manage subscription/i,\n  /manage (your )?preferences/i,\n  /update preferences/i,\n  /email preferences/i,\n  /unsubscribe/i,\n  /safely unsubscribe/i,\n  /why am i receiving/i,\n  /you are receiving/i,\n  /this email was sent/i,\n  /email subscriptions powered by/i,\n  /\\bpowered by\\b/i,\n  /\\bfeedblitz\\b/i,\n  /\\bmailchimp\\b/i,\n  /\\bsubstack\\b/i,\n  /\\bbeehiiv\\b/i,\n  /\\bconvertkit\\b/i,\n  /\\bcampaign monitor\\b/i,\n  /\\bconstant contact\\b/i,\n  /\\bklaviyo\\b/i,\n  /privacy policy/i,\n  /\\bterms\\b/i,\n];\n\nconst FOOTER_WEAK_TAIL = [\n  /\\barchives?\\b/i,\n  /\\bpreferences?\\b/i,\n  /\\bcontact\\b/i,\n  /\\bsubscribe\\b/i,\n];\n\nfunction lineLooksMostlyLinks(line) {\n  const l = String(line || \"\").trim();\n  if (!l) return false;\n\n  const urls = extractUrls(l);\n  if (!urls.length) return false;\n\n  const urlChars = urls.reduce((a, u) => a + u.length, 0);\n  const alpha = countLetters(l);\n  const total = Math.max(1, l.length);\n\n  const alphaRatio = alpha / total;\n  const linkDensity = urlChars / total;\n\n  // url-only counts as mostly links\n  if (lineIsUrlOnly(l)) return true;\n\n  return linkDensity > 0.60 && alphaRatio < 0.22;\n}\n\nfunction findFooterAnchor(lines) {\n  // use trimmed non-empty lines but keep original indices\n  let end = lines.length - 1;\n  while (end >= 0 && !String(lines[end] || \"\").trim()) end--;\n\n  if (end < 0) return null;\n\n  const TAIL = 80;\n  const start = Math.max(0, end - TAIL);\n\n  const minPos = Math.floor(lines.length * 0.55); // avoid clipping mid-body sections\n\n  let best = null; // { idx, score, line }\n  for (let i = start; i <= end; i++) {\n    const raw = String(lines[i] || \"\");\n    const t = raw.trim();\n    if (!t) continue;\n\n    let score = 0;\n\n    if (FOOTER_ANCHORS_STRONG.some((re) => re.test(t))) score += 6;\n    if (FOOTER_WEAK_TAIL.some((re) => re.test(t))) score += 2;\n\n    if (lineIsUrlOnly(t)) score += 2;\n    if (lineLooksMostlyLinks(t)) score += 1;\n\n    // require strong-ish evidence\n    if (score < 6) continue;\n    if (i < minPos) continue;\n\n    // choose the earliest strong anchor in tail (cuts full footer), but keep higher score if same idx\n    if (!best || i < best.idx || (i === best.idx && score > best.score)) {\n      best = { idx: i, score, line: t };\n    }\n  }\n\n  return best;\n}\n\nfunction trimFooterByAnchor(text) {\n  const orig = normalizeNewlines(String(text || \"\")).trim();\n  const lines = orig.split(\"\\n\");\n\n  const anchor = findFooterAnchor(lines);\n  if (!anchor) {\n    return { text: orig, trimmed: false, anchorFound: false, anchorLine: -1, removedLines: 0, reason: \"no_anchor\" };\n  }\n\n  const kept = lines.slice(0, anchor.idx).join(\"\\n\").trim();\n  const removedLines = Math.max(0, lines.length - anchor.idx);\n\n  return {\n    text: kept,\n    trimmed: true,\n    anchorFound: true,\n    anchorLine: anchor.idx,\n    anchorScore: anchor.score,\n    anchorPreview: anchor.line.slice(0, 180),\n    removedLines,\n    reason: \"anchor_cut\",\n  };\n}\n\nfunction trimFooterFallbackFromBottom(text) {\n  // conservative bottom-up fallback: remove only if clearly footer-ish, stop early\n  const orig = normalizeNewlines(String(text || \"\")).trim();\n  const lines = orig.split(\"\\n\");\n\n  let end = lines.length - 1;\n  while (end >= 0 && !String(lines[end] || \"\").trim()) end--;\n  if (end < 0) return { text: \"\", trimmed: false, removedLines: 0, reason: \"empty\" };\n\n  const footerish = (line) => {\n    const t = String(line || \"\").trim();\n    if (!t) return true;\n\n    // strong anchors => footerish\n    if (FOOTER_ANCHORS_STRONG.some((re) => re.test(t))) return true;\n\n    // weak anchors only if in tail and line is also linky/nav-ish\n    if (FOOTER_WEAK_TAIL.some((re) => re.test(t)) && (lineLooksMostlyLinks(t) || extractUrls(t).length > 0)) return true;\n\n    // purely url-only at bottom is usually footer/nav\n    if (lineIsUrlOnly(t) && lineLooksMostlyLinks(t)) return true;\n\n    return false;\n  };\n\n  let cut = end;\n  let removed = 0;\n\n  while (cut >= 0) {\n    const t = String(lines[cut] || \"\").trim();\n    if (!t) {\n      cut--;\n      continue;\n    }\n\n    if (footerish(t)) {\n      removed++;\n      cut--;\n      continue;\n    }\n    break;\n  }\n\n  if (removed === 0) {\n    return { text: orig, trimmed: false, removedLines: 0, reason: \"no_footerish_tail\" };\n  }\n\n  const out = lines.slice(0, cut + 1).join(\"\\n\").trim();\n  return { text: out, trimmed: true, removedLines: removed, reason: \"fallback_bottomup\" };\n}\n\n/* --------------------------- author derivation --------------------------- */\n\nfunction looksLikePersonName(s) {\n  const v = String(s || \"\").trim();\n  if (!v) return false;\n  if (v.length < 2 || v.length > 60) return false;\n  if (/https?:\\/\\//i.test(v) || /@/.test(v)) return false;\n  if (/[<>]/.test(v)) return false;\n  if (/^the\\b/i.test(v)) return false;\n  if (/^the way\\b/i.test(v)) return false;\n\n  // allow letters + spaces + a few name punctuations\n  if (!/^[A-Za-z\\u00C0-\\u024F\\u1E00-\\u1EFF\\s.'’-]+$/.test(v)) return false;\n\n  // avoid single long weird token\n  const words = v.split(/\\s+/).filter(Boolean);\n  if (words.length > 6) return false;\n\n  // require at least 1 letter\n  if (countLetters(v) < 2) return false;\n\n  return true;\n}\n\nfunction deriveAuthorFromTop(mdText) {\n  const lines = normalizeNewlines(String(mdText || \"\")).split(\"\\n\").slice(0, 25);\n  for (const ln of lines) {\n    const t = String(ln || \"\").trim();\n    if (!t) continue;\n\n    const m = t.match(/^by\\s+(.+)\\s*$/i);\n    if (!m) continue;\n\n    const cand = String(m[1] || \"\").trim();\n    if (!looksLikePersonName(cand)) continue;\n\n    return cand;\n  }\n  return null;\n}\n\n/* ------------------------------- pipeline ------------------------------- */\n\nconst mdInRaw = String($json.newsletter_md || \"\");\nconst mdInNorm = normalizeNewlines(mdInRaw);\n\nconst inputAlpha = countLetters(mdInNorm);\nconst inputLen = mdInNorm.trim().length;\nconst inputMeaningful = inputAlpha >= 180 || (inputLen >= 800 && inputAlpha >= 80);\n\n// Baseline sanitize (do NOT delete semantics)\nconst moji1 = fixMojibakeGuarded(mdInNorm);\nlet text = moji1.text;\ntext = decodeEntities(text);\ntext = stripInvisibleTransport(text);\ntext = collapseWhitespacePreserveNewlines(text);\n\n// Work with lines for removals\nlet lines = text.split(\"\\n\");\n\nlet debug_transport_removed = 0;\nlet debug_empty_angle_removed = 0;\nlet debug_separators_removed = 0;\n\nlines = lines.filter((ln) => {\n  if (isTransportArtifactLine(ln)) {\n    debug_transport_removed++;\n    return false;\n  }\n  if (isEmptyAngleLink(ln)) {\n    debug_empty_angle_removed++;\n    return false;\n  }\n  if (isDecorativeSeparator(ln)) {\n    debug_separators_removed++;\n    return false;\n  }\n  return true;\n});\n\n// View Online (strict)\nconst hasUrlCanonicalAlready = $json.url_canonical && String($json.url_canonical).trim() !== \"\";\nconst viewOnline = tryExtractViewOnline(lines);\nlet canonical_url = null;\nlet debug_view_online_removed = false;\n\nif (viewOnline.found && viewOnline.removed && viewOnline.url) {\n  canonical_url = viewOnline.url;\n  debug_view_online_removed = true;\n}\n\n// If url_canonical already exists, do not overwrite\nconst url_canonical = hasUrlCanonicalAlready ? String($json.url_canonical).trim() : (canonical_url || null);\n\n// Remove empty lines created by label/url deletion\nlines = lines.filter((ln) => String(ln || \"\").trim() !== \"\");\n\n// FeedBlitz tracking removal (safe)\nconst fb = removeFeedblitzTracking(lines);\nlines = fb.lines;\n\n// Rebuild candidate pre-footer text\nlet v1 = collapseWhitespacePreserveNewlines(lines.join(\"\\n\"));\n\n// Optional VERY-strong teaser drop (usually disabled in practice)\n// Only attempt if Node1 did not already drop teaser.\nlet debug_teaser_removed = false;\nlet debug_teaser_removed_reason = \"disabled_or_not_triggered\";\nlet debug_teaser_removed_span = \"\";\n\nconst upstreamTeaserDropped = $json.debug_teaser_dropped === true;\n\nif (!upstreamTeaserDropped) {\n  const td = maybeDropTeaserVeryStrong(v1);\n  if (td.dropped) {\n    v1 = td.text;\n    debug_teaser_removed = true;\n    debug_teaser_removed_reason = td.reason;\n    debug_teaser_removed_span = td.removed;\n  } else {\n    debug_teaser_removed_reason = td.reason;\n  }\n}\n\n// Footer trim (anchor-first, fallback-second)\nconst footerAnchorTrim = trimFooterByAnchor(v1);\nlet v2 = footerAnchorTrim.text;\n\nlet footerFallbackTrim = null;\nif (!footerAnchorTrim.trimmed) {\n  footerFallbackTrim = trimFooterFallbackFromBottom(v1);\n  v2 = footerFallbackTrim.text;\n}\n\n// Polish: collapse whitespace\nv2 = collapseWhitespacePreserveNewlines(v2);\n\n// Safety: NEVER empty (unless input empty)\nlet clean_text = v2;\n\nlet debug_reverted_due_to_small_output = false;\nlet debug_revert_reason = \"none\";\nlet debug_final_stage = \"footer_trimmed\";\n\nconst outLen = clean_text.trim().length;\nconst outAlpha = countLetters(clean_text);\n\nconst tooSmall =\n  (inputLen >= 1200 && outLen < Math.max(260, Math.floor(inputLen * 0.12))) ||\n  (inputLen >= 800 && outLen < 160) ||\n  (inputMeaningful && outAlpha < Math.min(60, Math.floor(inputAlpha * 0.10))) ||\n  (inputMeaningful && outLen === 0);\n\nif (tooSmall) {\n  // revert to less aggressive version (pre-footer)\n  const v1Len = v1.trim().length;\n  const v1Alpha = countLetters(v1);\n\n  clean_text = v1;\n\n  debug_reverted_due_to_small_output = true;\n  debug_revert_reason = `too_small_after_footer_trim(outLen=${outLen},inLen=${inputLen})`;\n  debug_final_stage = \"reverted_to_pre_footer\";\n\n  // If even v1 is somehow empty but input had content, fall back to baseline text\n  if (clean_text.trim().length === 0 && inputLen > 0) {\n    clean_text = text.trim();\n    debug_revert_reason += \"; fallback_to_baseline\";\n    debug_final_stage = \"reverted_to_baseline\";\n  }\n\n  // If view-online removal was the only meaningful deletion (rare), keep safety\n  if (inputMeaningful && v1Len < Math.floor(inputLen * 0.10) && v1Alpha < Math.floor(inputAlpha * 0.10)) {\n    clean_text = text.trim();\n    debug_revert_reason += \"; v1_too_small_fallback_to_baseline\";\n    debug_final_stage = \"reverted_to_baseline\";\n  }\n}\n\n// Final hard requirement: never empty unless input empty/whitespace\nif (clean_text.trim().length === 0 && inputLen > 0) {\n  clean_text = text.trim();\n  debug_reverted_due_to_small_output = true;\n  debug_revert_reason = debug_revert_reason === \"none\" ? \"hard_non_empty_fallback\" : debug_revert_reason + \"; hard_non_empty_fallback\";\n  debug_final_stage = \"hard_fallback\";\n}\n\n// Author derivation (do NOT override existing)\nlet author = $json.author && String($json.author).trim() !== \"\" ? String($json.author).trim() : null;\nlet debug_author_derived = false;\n\nif (!author) {\n  const derived = deriveAuthorFromTop(clean_text);\n  if (derived) {\n    author = derived;\n    debug_author_derived = true;\n  }\n}\n\nreturn [\n  {\n    json: {\n      ...$json,\n\n      // outputs\n      clean_text,\n      author,\n      url_canonical,\n\n      node3_at: nowIso,\n\n      // debug\n      debug_node3_input_len: inputLen,\n      debug_node3_input_alpha: inputAlpha,\n      debug_node3_output_len: clean_text.trim().length,\n      debug_node3_output_alpha: countLetters(clean_text),\n\n      debug_mojibake_fixed: !!moji1.fixed,\n      debug_mojibake_method: moji1.method,\n\n      debug_transport_lines_removed: debug_transport_removed,\n      debug_empty_angle_lines_removed: debug_empty_angle_removed,\n      debug_decorative_separators_removed: debug_separators_removed,\n\n      debug_view_online_found: !!viewOnline.found,\n      debug_view_online_removed: !!debug_view_online_removed,\n      debug_view_online_reason: viewOnline.reason,\n      debug_view_online_line: viewOnline.at,\n      debug_view_online_url: canonical_url || null,\n\n      debug_feedblitz_tracking_lines_removed: fb.removedLines,\n      debug_feedblitz_tracking_urls_stripped: fb.strippedUrls,\n\n      debug_footer_anchor_found: !!footerAnchorTrim.anchorFound,\n      debug_footer_anchor_line: footerAnchorTrim.anchorFound ? footerAnchorTrim.anchorLine : -1,\n      debug_footer_anchor_score: footerAnchorTrim.anchorFound ? footerAnchorTrim.anchorScore : 0,\n      debug_footer_anchor_preview: footerAnchorTrim.anchorFound ? footerAnchorTrim.anchorPreview : \"\",\n      debug_footer_trimmed: !!(footerAnchorTrim.trimmed || (footerFallbackTrim && footerFallbackTrim.trimmed)),\n      debug_footer_trim_reason: footerAnchorTrim.trimmed\n        ? footerAnchorTrim.reason\n        : footerFallbackTrim\n        ? footerFallbackTrim.reason\n        : \"none\",\n\n      debug_teaser_removed: !!debug_teaser_removed,\n      debug_teaser_removed_reason: debug_teaser_removed_reason,\n      debug_teaser_removed_span: debug_teaser_removed ? debug_teaser_removed_span.slice(0, 4000) : \"\",\n\n      debug_reverted_due_to_small_output: !!debug_reverted_due_to_small_output,\n      debug_revert_reason: debug_revert_reason,\n      debug_final_stage: debug_final_stage,\n\n      debug_author_derived: debug_author_derived,\n    },\n  },\n];\n"
      },
      "position": [
        1024,
        -576
      ],
      "type": "n8n-nodes-base.code",
      "typeVersion": 2
    },
    {
      "id": "344c864b-cc78-4f3c-b10a-c7cf2f1050dd",
      "name": "Normalization1",
      "parameters": {
        "jsCode": "// Correspondence Node 1: Prepare text\n\nconst normalizeNewlines = (s) => String(s || '').replace(/\\r\\n/g, '\\n').replace(/\\r/g, '\\n');\nconst stripInvisible = (s) =>\n  String(s || '')\n    .replace(/[\\u200B-\\u200D\\uFEFF\\u00AD\\u2060\\u034F]/g, '')\n    .replace(/\\u00A0/g, ' ')\n    .trim();\n\nlet t = $json.core_text || $json.capture_text || '';\nt = stripInvisible(normalizeNewlines(t));\n\n// Drop common “single-line” noise seen in your Outlook sample\nconst dropLine = [\n  /^proprietary\\s*$/i, // seen in sample thread:contentReference[oaicite:21]{index=21}\n  /^get outlook for ios$/i, // repeated in sample:contentReference[oaicite:22]{index=22}\n  /^caution:\\s*this email originated from outside/i, // seen in sample:contentReference[oaicite:23]{index=23}\n  /^links contained in this email have been replaced by zixprotect/i, // seen in sample:contentReference[oaicite:24]{index=24}\n];\n\nt = t\n  .split('\\n')\n  .filter(line => !dropLine.some(re => re.test(line.trim())))\n  .join('\\n')\n  .replace(/\\n{3,}/g, '\\n\\n')\n  .trim();\n\nreturn [{ json: { ...$json, corr_text: t } }];\n"
      },
      "position": [
        576,
        -384
      ],
      "type": "n8n-nodes-base.code",
      "typeVersion": 2
    },
    {
      "id": "00fb2628-a0f9-40a0-9307-ba07a9fcc25a",
      "name": "Thread Parser",
      "parameters": {
        "jsCode": "// Correspondence Node 2: Parse Outlook-style thread into message blocks\n\nconst text = String($json.corr_text || '');\n\nfunction parseOutlookBlocks(t) {\n  const blocks = [];\n  // Find all occurrences of a header starting at line-begin\n  const re = /^From:\\s*(.+)\\nSent:\\s*(.+)\\nTo:\\s*(.+)\\nSubject:\\s*(.+)\\s*$/gim;\n\n  const matches = [];\n  let m;\n  while ((m = re.exec(t)) !== null) {\n    matches.push({ idx: m.index, from: m[1], sent: m[2], to: m[3], subject: m[4] });\n  }\n\n  if (matches.length === 0) return null;\n\n  for (let i = 0; i < matches.length; i++) {\n    const cur = matches[i];\n    const nextIdx = (i + 1 < matches.length) ? matches[i + 1].idx : t.length;\n    const startBodyIdx = cur.idx + (t.slice(cur.idx).match(re)?.[0]?.length || 0);\n\n    const body = t.slice(startBodyIdx, nextIdx).trim();\n\n    blocks.push({\n      from: cur.from.trim(),\n      sent: cur.sent.trim(),\n      to: cur.to.trim(),\n      subject: cur.subject.trim(),\n      body\n    });\n  }\n\n  return blocks;\n}\n\nconst blocks = parseOutlookBlocks(text);\n\n// Fallback: single block if not parseable\nconst parsed = blocks || [{ from: $json.author || '', sent: '', to: '', subject: $json.title || '', body: text }];\n\nreturn [{ json: { ...$json, corr_blocks: parsed } }];\n"
      },
      "position": [
        800,
        -384
      ],
      "type": "n8n-nodes-base.code",
      "typeVersion": 2
    },
    {
      "id": "46831d4c-c2ac-42a8-823f-9947d36c5201",
      "name": "Signatures and Markdown",
      "parameters": {
        "jsCode": "// Correspondence Node 3: Remove signature-ish tails + format as Markdown\n\nconst blocks = Array.isArray($json.corr_blocks) ? $json.corr_blocks : [];\n\nconst stripInvisible = (s) =>\n  String(s || '')\n    .replace(/[\\u200B-\\u200D\\uFEFF\\u00AD\\u2060\\u034F]/g, '')\n    .replace(/\\u00A0/g, ' ');\n\nfunction stripSignature(body) {\n  let lines = stripInvisible(body).replace(/\\r\\n/g, '\\n').replace(/\\r/g, '\\n').split('\\n');\n\n  // Drop common signature/disclaimer “sections”\n  const killIf = [\n    /^get outlook for ios$/i,\n    /^follow us\\b/i,\n    /zixprotect/i,\n    /^caution:\\s*this email originated from outside/i,\n    /^\\[cid:/i,\n  ];\n\n  // Remove lines that match killIf anywhere\n  lines = lines.filter(l => !killIf.some(re => re.test(l.trim())));\n\n  // Cut off “contact-card” tails (heuristic)\n  // If we see multiple contact-ish lines near the end, cut there.\n  const contactish = (l) =>\n    /(phone|mobile|tel|fax|address|website|linkedin|ht?ecgroup|@)/i.test(l) ||\n    /\\+?\\d[\\d\\s().-]{7,}\\d/.test(l); // phone-like\n\n  for (let i = Math.max(0, lines.length - 25); i < lines.length; i++) {\n    if (contactish(lines[i])) {\n      const tail = lines.slice(i).filter(x => x.trim() !== '');\n      const tailContactCount = tail.filter(contactish).length;\n      if (tailContactCount >= 2) {\n        lines = lines.slice(0, i);\n        break;\n      }\n    }\n  }\n\n  return lines.join('\\n').replace(/\\n{3,}/g, '\\n\\n').trim();\n}\n\nfunction fmtBlock(b, idx) {\n  const header = [\n    `### Message ${idx + 1}`,\n    b.from ? `- From: ${b.from}` : null,\n    b.to ? `- To: ${b.to}` : null,\n    b.sent ? `- Sent: ${b.sent}` : null,\n    b.subject ? `- Subject: ${b.subject}` : null,\n  ].filter(Boolean).join('\\n');\n\n  const body = stripSignature(b.body || '');\n  return `${header}\\n\\n${body}`.trim();\n}\n\nconst parts = blocks.map(fmtBlock).filter(Boolean);\n\nconst threadMd = parts.join('\\n\\n---\\n\\n').trim();\n\nreturn [{ json: { ...$json, clean_text: threadMd } }];\n"
      },
      "position": [
        1024,
        -384
      ],
      "type": "n8n-nodes-base.code",
      "typeVersion": 2
    },
    {
      "credentials": {
        "imap": {
          "id": "3oE0cH2d8fpvD7IC",
          "name": "Google (pkm.gasovic)"
        }
      },
      "id": "eb003274-ba9f-420a-96da-2d5092842039",
      "name": "Email Trigger (IMAP)1",
      "parameters": {
        "options": {}
      },
      "position": [
        -304,
        -480
      ],
      "type": "n8n-nodes-base.emailReadImap",
      "typeVersion": 2.1
    },
    {
      "id": "0d46a330-acb5-4954-a88e-1e0ee38ac0cf",
      "name": "Capture",
      "notes": "## 1) Capture\n- Ingest raw email content and metadata (subject/from/date/message-id), without losing fidelity.\n- Preserve original MIME/HTML/text parts (or raw .eml) for later fallbacks and debugging.\n- Record provenance fields (source mailbox, label/folder, received timestamp, thread/forward info).\n- Output a structured “raw payload” object: {headers, raw_body, parts, attachments, ids}.\n",
      "parameters": {
        "jsCode": "/**\n * Node0 — Transport normalization + lightweight metadata (capture_text -> core_text/core_html)\n *\n * CONTRACT\n * Input (IMAP-ish):\n *   - subject/from + text/plain + text/html (optional)\n *\n * Output (downstream):\n *   - capture_text: normalized newlines, otherwise “as received”\n *   - core_text: transport-cleaned body (forwarded wrapper headers removed, invisible junk stripped, mojibake repaired if detected)\n *   - core_html: HTML with forwarded wrapper header removed (best-effort, non-destructive)\n *   - metadata: title, author, intent, content_type\n *   - debug: has_html/has_plain, lens, forwarded parser, mojibake fixed flags\n *\n * IMPORTANT\n *   - NO teaser/footer/main-content selection here.\n *   - No external deps; stock n8n runner compatible.\n */\n\nconst nowIso = new Date().toISOString();\n\n/* ----------------------------- small helpers ----------------------------- */\n\nconst pick = (...vals) => {\n  for (const v of vals) {\n    if (v === null || v === undefined) continue;\n    const s = String(v);\n    if (s.trim() !== \"\") return v;\n  }\n  return null;\n};\n\nconst normalizeNewlines = (s) => String(s || \"\").replace(/\\r\\n/g, \"\\n\").replace(/\\r/g, \"\\n\");\n\n// Minimal entity decode (text nodes sometimes contain these)\nconst decodeEntities = (s) =>\n  String(s || \"\")\n    .replace(/&nbsp;/gi, \" \")\n    .replace(/&#160;/g, \" \")\n    .replace(/&amp;/g, \"&\")\n    .replace(/&lt;/g, \"<\")\n    .replace(/&gt;/g, \">\")\n    .replace(/&quot;/g, '\"')\n    .replace(/&#39;/g, \"'\")\n    .replace(/&apos;/g, \"'\");\n\n// Runtime-safe “strip Unicode Cf”\nconst makeCfRegex = () => {\n  try {\n    return new RegExp(\"\\\\p{Cf}+\", \"gu\"); // format chars (ZWJ/ZWNJ/etc)\n  } catch {\n    // Fallback: common newsletter offenders\n    return /[\\u200B-\\u200F\\u202A-\\u202E\\u2060-\\u206F\\uFEFF\\u00AD\\u034F]/g;\n  }\n};\nconst RE_CF = makeCfRegex();\n\n// Normalize “weird” Unicode spaces into normal space (helps with \\u202F etc)\nconst RE_UNI_SPACE = /[\\u00A0\\u1680\\u2000-\\u200A\\u2007\\u202F\\u205F\\u3000]/g;\n\n// Mojibake (guarded) — same spirit as your Node2, but runs earlier for core_text\nconst MOJI_SIGNAL = /(?:â[\\u0080-\\u00BF]|Ã[\\u0080-\\u00BF]|Â[\\u0080-\\u00BF]|â€”|â€“|â€™|â€œ|â€\\x9d|â€¢|â€¦|Â )/;\n\nfunction mojiScore(s) {\n  const str = String(s || \"\");\n  if (!str) return 0;\n  const m = str.match(new RegExp(MOJI_SIGNAL.source, \"g\"));\n  const scoreSignals = m ? m.length : 0;\n  const scoreRepl = (str.match(/\\uFFFD/g) || []).length; // replacement char\n  return scoreSignals + scoreRepl;\n}\n\nfunction fixMojibakeGuarded(s) {\n  const str = String(s || \"\");\n  if (!str) return { text: str, fixed: false, method: \"none\" };\n\n  // Strong guard: only attempt when there’s a clear mojibake signal\n  if (!MOJI_SIGNAL.test(str) && str.indexOf(\"\\uFFFD\") === -1) {\n    return { text: str, fixed: false, method: \"none\" };\n  }\n\n  // Attempt: interpret current string as Latin-1 bytes and decode as UTF-8\n  try {\n    const bytes = Uint8Array.from(str, (c) => c.charCodeAt(0) & 0xff);\n\n    let out = null;\n\n    if (typeof TextDecoder !== \"undefined\") {\n      out = new TextDecoder(\"utf-8\", { fatal: false }).decode(bytes);\n    } else if (typeof Buffer !== \"undefined\") {\n      out = Buffer.from(bytes).toString(\"utf8\");\n    }\n\n    if (out && out !== str) {\n      // Accept if it doesn’t get “more mojibakey”\n      if (mojiScore(out) <= mojiScore(str)) {\n        return { text: out, fixed: true, method: \"latin1->utf8\" };\n      }\n    }\n  } catch (_) {}\n\n  // Fallback: best-effort replacements of the most common sequences\n  const out2 = str\n    .replace(/â€™/g, \"’\")\n    .replace(/â€œ/g, \"“\")\n    .replace(/â€\\x9d/g, \"”\")\n    .replace(/â€“/g, \"–\")\n    .replace(/â€”/g, \"—\")\n    .replace(/â€¦/g, \"…\")\n    .replace(/â€¢/g, \"•\")\n    .replace(/Â /g, \" \")\n    .replace(/Â/g, \"\"); // last-resort: stray Â\n  return { text: out2, fixed: out2 !== str, method: out2 !== str ? \"replace\" : \"none\" };\n}\n\nfunction stripInvisibleTransport(s) {\n  // transport-ish cleanup only\n  return String(s || \"\")\n    .replace(/\\u0000/g, \"\") // NUL\n    .replace(RE_UNI_SPACE, \" \")\n    .replace(RE_CF, \"\")\n    .replace(/\\uFFFD/g, \"\") // replacement char is rarely useful in stored text\n    .replace(/\\u00A0/g, \" \"); // NBSP -> space (redundant w/ RE_UNI_SPACE; harmless)\n}\n\n// Preserve newlines, but normalize whitespace INSIDE lines, cap blank-line runs\nfunction collapseWhitespacePreserveNewlines(s) {\n  const text = normalizeNewlines(s);\n  const lines = text.split(\"\\n\");\n\n  const out = [];\n  let blankRun = 0;\n\n  for (let line of lines) {\n    line = String(line || \"\")\n      .replace(/[ \\t]+/g, \" \")\n      .trim();\n\n    if (!line) {\n      blankRun += 1;\n      if (blankRun <= 2) out.push(\"\");\n      continue;\n    }\n\n    blankRun = 0;\n    out.push(line);\n  }\n\n  return out.join(\"\\n\").trim();\n}\n\nconst normalizeSubject = (subj) =>\n  String(subj || \"\")\n    .replace(/^\\s*((re|fw|fwd)\\s*:\\s*)+/i, \"\")\n    .trim() || null;\n\nconst nameOnly = (fromVal) => {\n  if (!fromVal) return null;\n  let s = String(fromVal);\n\n  // Prefer display name before <email>\n  const m = s.match(/^\\s*\"?([^\"<]+?)\"?\\s*<[^>]+>\\s*$/);\n  if (m && m[1]) s = m[1];\n\n  // Decode + repair + strip transport junk\n  s = decodeEntities(s);\n  s = fixMojibakeGuarded(s).text;\n  s = stripInvisibleTransport(s);\n\n  s = s.replace(/[\"“”<>]/g, \" \").replace(/\\s+/g, \" \").trim();\n\n  // Keep letters/numbers/space/hyphen only (unicode-safe). If property escapes fail, fall back.\n  try {\n    s = s.replace(/[^\\p{L}\\p{N}\\s-]+/gu, \"\").replace(/\\s+/g, \" \").trim();\n  } catch {\n    s = s.replace(/[^A-Za-z0-9\\s-]+/g, \"\").replace(/\\s+/g, \" \").trim();\n  }\n\n  return s || null;\n};\n\n/* ---------------------- forwarded wrapper handling (plain) ---------------------- */\n\nfunction parseMiniHeaderBlock(lines, startIdx, opts) {\n  const maxLines = (opts && opts.maxLines) || 30;\n  const keys = /^(From|To|Subject|Date|Sent|Cc|Bcc|Reply-To):\\s*(.*)\\s*$/i;\n\n  let i = startIdx;\n  let skipped = 0;\n  while (i < lines.length && skipped < 3 && String(lines[i] || \"\").trim() === \"\") {\n    i++;\n    skipped++;\n  }\n\n  const headers = {};\n  const wrapperLines = [];\n  let lastKey = null;\n  let seenBlankTerminator = false;\n\n  for (let n = 0; i < lines.length && n < maxLines; i++, n++) {\n    const line = String(lines[i] || \"\");\n    wrapperLines.push(line);\n\n    if (line.trim() === \"\") {\n      seenBlankTerminator = true;\n      i++; // body starts after the blank line\n      break;\n    }\n\n    const hm = line.match(keys);\n    if (hm) {\n      lastKey = hm[1].toLowerCase().replace(\"-\", \"_\"); // reply-to -> reply_to\n      const val = String(hm[2] || \"\").trim();\n      headers[lastKey] = headers[lastKey] ? (headers[lastKey] + \" \" + val).trim() : val;\n      continue;\n    }\n\n    // Continuation line\n    if (lastKey && /^\\s+/.test(line)) {\n      headers[lastKey] = (headers[lastKey] + \" \" + line.trim()).trim();\n    }\n  }\n\n  const score =\n    (headers.from ? 1 : 0) +\n    (headers.to ? 1 : 0) +\n    (headers.subject ? 1 : 0) +\n    (headers.date || headers.sent ? 1 : 0);\n\n  const found = seenBlankTerminator && score >= ((opts && opts.minScore) || 2);\n\n  return { found, headers, wrapperLines, bodyStart: i, score, seenBlankTerminator };\n}\n\nfunction stripTopHeaderBlockIfPresent(text) {\n  const t = normalizeNewlines(text);\n  const lines = t.split(\"\\n\");\n\n  // Find first non-empty line near top\n  let first = -1;\n  for (let i = 0; i < Math.min(lines.length, 25); i++) {\n    if (String(lines[i] || \"\").trim() !== \"\") {\n      first = i;\n      break;\n    }\n  }\n  if (first === -1) return { text: t.trim(), stripped: false, parsed: null };\n\n  // For “header-only” stripping, be stricter than marker-based forwarding\n  const hb = parseMiniHeaderBlock(lines, first, { minScore: 3, maxLines: 25 });\n  if (!hb.found) return { text: t.trim(), stripped: false, parsed: null };\n\n  const remaining = lines.slice(hb.bodyStart).join(\"\\n\").trim();\n  // Safety: don’t nuke tiny content\n  if (remaining.length < 80) return { text: t.trim(), stripped: false, parsed: null };\n\n  return {\n    text: remaining,\n    stripped: true,\n    parsed: { headers: hb.headers, wrapper_text: hb.wrapperLines.join(\"\\n\").trim(), score: hb.score },\n  };\n}\n\nfunction extractForwardedPlainText(raw) {\n  const text = normalizeNewlines(raw);\n  const lines = text.split(\"\\n\");\n\n  const MAX_SCAN = 80;\n\n  const MARKERS = [\n    { name: \"gmail\", re: /^-+\\s*Forwarded message\\s*-+\\s*$/i },\n    { name: \"gmail_begin\", re: /^Begin forwarded message:\\s*$/i },\n    { name: \"outlook\", re: /^-+\\s*Original Message\\s*-+\\s*$/i },\n    { name: \"generic_fwd\", re: /^-+\\s*Forwarded\\s+Message\\s*-+\\s*$/i },\n    { name: \"generic_fwd2\", re: /^-{2,}\\s*Forwarded\\s+message\\s*-{2,}\\s*$/i },\n  ];\n\n  let idx = -1;\n  let parser = \"none\";\n  let marker_line = \"\";\n\n  for (let i = 0; i < Math.min(lines.length, MAX_SCAN); i++) {\n    const t = String(lines[i] || \"\").trim();\n    for (const m of MARKERS) {\n      if (m.re.test(t)) {\n        idx = i;\n        parser = m.name;\n        marker_line = lines[i] || \"\";\n        break;\n      }\n    }\n    if (idx !== -1) break;\n  }\n\n  // No explicit marker — try strict header-only stripping if it starts with From:/To:/Subject:/Date\n  if (idx === -1) {\n    const stripped = stripTopHeaderBlockIfPresent(text);\n    if (stripped.stripped) {\n      return {\n        found: true,\n        parser: \"header_block\",\n        marker_line: null,\n        preamble: \"\",\n        headers: stripped.parsed.headers,\n        wrapper_text: stripped.parsed.wrapper_text,\n        body: stripped.text,\n      };\n    }\n\n    return {\n      found: false,\n      parser: \"none\",\n      marker_line: null,\n      preamble: \"\",\n      headers: {},\n      wrapper_text: \"\",\n      body: text.trim(),\n    };\n  }\n\n  // Marker found — require a recognizable mini-header block right after it\n  const preamble = lines.slice(0, idx).join(\"\\n\").trim();\n  const hb = parseMiniHeaderBlock(lines, idx + 1, { minScore: 2, maxLines: 35 });\n\n  if (!hb.found) {\n    // Marker without headers: don’t strip; too risky.\n    return {\n      found: false,\n      parser: \"none\",\n      marker_line: null,\n      preamble: \"\",\n      headers: {},\n      wrapper_text: \"\",\n      body: text.trim(),\n    };\n  }\n\n  let body = lines.slice(hb.bodyStart).join(\"\\n\").trim();\n\n  // Safety: if body still starts with mini headers, strip them (rare)\n  body = body.replace(/^(?:\\s*(?:From|To|Subject|Date|Sent):[^\\n]*\\n){1,10}\\s*\\n?/i, \"\").trim();\n\n  return {\n    found: true,\n    parser,\n    marker_line,\n    preamble,\n    headers: hb.headers,\n    wrapper_text: [marker_line, hb.wrapperLines.join(\"\\n\")].filter(Boolean).join(\"\\n\").trim(),\n    body,\n  };\n}\n\n/* ---------------------- forwarded wrapper handling (html) ---------------------- */\n\nfunction stripForwardedHtmlHeaders(html) {\n  let h = String(html || \"\");\n  if (!h) return null;\n\n  // Remove scripts/styles (transport noise)\n  h = h.replace(/<script[\\s\\S]*?<\\/script>/gi, \"\");\n  h = h.replace(/<style[\\s\\S]*?<\\/style>/gi, \"\");\n\n  // Gmail forwarded header block\n  h = h.replace(\n    /<div[^>]*class=[\"'][^\"']*gmail_attr[^\"']*[\"'][^>]*>[\\s\\S]*?<\\/div>\\s*(<br\\s*\\/?>\\s*){0,6}/i,\n    \"\"\n  );\n\n  // Plain marker text sometimes appears outside gmail_attr\n  h = h.replace(/-+\\s*Forwarded message\\s*-+/gi, \"\");\n\n  // Outlook-ish “Original Message” header blocks (best-effort, conservative)\n  // Only removes a short top block with multiple From/To/Subject lines separated by <br>\n  h = h.replace(\n    /(?:<br\\s*\\/?>\\s*){0,4}[-_]{2,}\\s*(?:Original Message|Forwarded message)\\s*[-_]{2,}\\s*(?:<br\\s*\\/?>\\s*(?:From|To|Subject|Date|Sent):[^<]{0,300}){2,12}(?:<br\\s*\\/?>\\s*){1,6}/i,\n    \"\"\n  );\n\n  // Remove hidden preheaders (display:none) — common transport artifact\n  h = h.replace(\n    /<[^>]+style=[\"'][^\"']*display\\s*:\\s*none[^\"']*[\"'][^>]*>[\\s\\S]*?<\\/[^>]+>/gi,\n    \"\"\n  );\n\n  return h;\n}\n\n/* ------------------------------ intent/content_type ----------------------------- */\n\n// THINK detection: first non-empty line is \"think\" or \"think:\" (case-insensitive)\nfunction applyThink(text) {\n  const lines = normalizeNewlines(text).split(\"\\n\");\n\n  let firstIdx = -1;\n  for (let i = 0; i < lines.length; i++) {\n    if (String(lines[i] || \"\").trim() !== \"\") {\n      firstIdx = i;\n      break;\n    }\n  }\n  if (firstIdx === -1) return { intent: \"archive\", text: \"\" };\n\n  const first = String(lines[firstIdx] || \"\").trim();\n  const m = first.match(/^think\\s*:?\\s*(.*)$/i);\n  if (!m) return { intent: \"archive\", text };\n\n  const remainder = String(m[1] || \"\").trim();\n  if (remainder) {\n    lines[firstIdx] = remainder;\n  } else {\n    lines.splice(firstIdx, 1);\n  }\n  return { intent: \"think\", text: lines.join(\"\\n\").trim() };\n}\n\n// Correspondence detection (heuristic)\nfunction looksLikeThread(text) {\n  const t = String(text || \"\");\n  if (/^From:\\s.+\\nSent:\\s.+\\nTo:\\s.+\\nSubject:\\s.+/im.test(t)) return true;\n  if (/^On .+wrote:\\s*$/im.test(t)) return true;\n  if (/^\\s*>/m.test(t)) return true;\n  if (/^_{8,}\\s*$/m.test(t)) return true;\n  return false;\n}\n\n/* ----------------------------------- inputs ----------------------------------- */\n\nconst subject_raw = pick($json.subject, $json.headers && $json.headers.subject);\nconst from_raw = pick($json.from && $json.from.text, $json.from, $json.headers && $json.headers.from);\n\nconst text_raw = pick(\n  $json.text,\n  $json.textPlain,\n  $json.body && $json.body.text,\n  $json.body,\n  $json.textContent\n) || \"\";\n\nconst html_raw = pick($json.html, $json.textHtml, $json.body && $json.body.html, $json.htmlContent);\n\n// capture_text: as received (except newline normalization)\nconst capture_text = normalizeNewlines(String(text_raw || \"\"));\n\n// Debug presence\nconst debug_has_plain = !!String(text_raw || \"\").trim();\nconst debug_has_html = !!String(html_raw || \"\").trim();\nconst debug_raw_plain_len = String(text_raw || \"\").length;\nconst debug_raw_html_len = String(html_raw || \"\").length;\n\n/* ------------------------------ build core_text ------------------------------ */\n\n// Split forwarded wrapper (plain)\nconst fwd = extractForwardedPlainText(capture_text);\n\n// Title: prefer forwarded Subject if present, else envelope subject\n{\n  const subjCandidate = fwd.headers.subject || subject_raw || \"\";\n  const repaired = fixMojibakeGuarded(stripInvisibleTransport(decodeEntities(subjCandidate)));\n  var title = normalizeSubject(repaired.text);\n  var debug_title_mojibake_fixed = repaired.fixed;\n}\n\n// Author: prefer forwarded From if present, else envelope from\n{\n  const fromCandidate = fwd.headers.from || from_raw || \"\";\n  const repairedFrom = fixMojibakeGuarded(String(fromCandidate || \"\"));\n  var author = nameOnly(repairedFrom.text);\n  var debug_author_mojibake_fixed = repairedFrom.fixed;\n}\n\n// core_text_raw: remove forwarded marker + mini-header wrapper, keep preamble above it\nlet core_text_raw = fwd.found\n  ? [fwd.preamble, fwd.body].filter(Boolean).join(\"\\n\\n\").trim()\n  : capture_text.trim();\n\n// If a header block still leaked to top, strip it strictly\nconst topStrip = stripTopHeaderBlockIfPresent(core_text_raw);\ncore_text_raw = topStrip.text;\n\n// Mojibake repair (guarded) on core text (this is the main change vs your current node0)\nconst coreMoji = fixMojibakeGuarded(core_text_raw);\nlet core_text_clean = coreMoji.text;\n\n// Transport cleanup: invisibles, unicode spaces, replacement chars, whitespace capping\ncore_text_clean = stripInvisibleTransport(decodeEntities(core_text_clean));\ncore_text_clean = collapseWhitespacePreserveNewlines(core_text_clean);\n\n// THINK intent (does not do semantic cleanup; just routes notes)\nconst thinkApplied = applyThink(core_text_clean);\nconst intent = thinkApplied.intent;\nlet core_text = thinkApplied.text;\n\n/* ------------------------------ build core_html ------------------------------ */\n\nlet core_html = stripForwardedHtmlHeaders(html_raw);\nif (core_html) {\n  // Keep this transport-level: repair mojibake only if signaled; strip invisibles; normalize newlines\n  const htmlMoji = fixMojibakeGuarded(core_html);\n  core_html = htmlMoji.text;\n  core_html = stripInvisibleTransport(core_html);\n  core_html = normalizeNewlines(core_html).trim();\n  if (!core_html) core_html = null;\n\n  var debug_core_html_mojibake_fixed = !!htmlMoji.fixed;\n} else {\n  var debug_core_html_mojibake_fixed = false;\n}\n\n/* ------------------------------ content_type -------------------------------- */\n\nlet content_type;\nif (intent === \"think\") {\n  content_type = \"note\";\n} else {\n  content_type = looksLikeThread(core_text) ? \"correspondence\" : \"newsletter\";\n}\n\n/* ----------------------------------- output ---------------------------------- */\n\nreturn [\n  {\n    json: {\n      ...$json,\n\n      source: \"email\",\n      created_at: nowIso,\n\n      // stored fields\n      intent,\n      content_type,\n      title,\n      author,\n      capture_text,\n      core_text,\n      core_html,\n\n      // forwarded wrapper split (debug/metadata; not used as content downstream)\n      forwarded_found: fwd.found,\n      forwarded_headers: fwd.headers,\n      forwarded_wrapper_text: fwd.wrapper_text,\n\n      // Debug signals / metrics for downstream decisions & regression auditing\n      debug_has_plain,\n      debug_has_html,\n      debug_raw_plain_len,\n      debug_raw_html_len,\n\n      debug_capture_len: capture_text.length,\n      debug_core_len: core_text.length,\n      debug_core_html_len: core_html ? core_html.length : 0,\n\n      debug_forwarded_parser: fwd.parser,          // gmail | outlook | header_block | none\n      debug_forwarded_marker_line: fwd.marker_line,\n      debug_forwarded_header_keys: Object.keys(fwd.headers || {}),\n      debug_top_header_stripped: !!topStrip.stripped,\n\n      debug_mojibake_fixed: !!coreMoji.fixed,\n      debug_mojibake_method: coreMoji.method,\n\n      debug_title_mojibake_fixed,\n      debug_author_mojibake_fixed,\n      debug_core_html_mojibake_fixed,\n    },\n  },\n];\n"
      },
      "position": [
        128,
        -480
      ],
      "type": "n8n-nodes-base.code",
      "typeVersion": 2
    },
    {
      "id": "b74a02d7-1371-4de6-afc4-4499b981e469",
      "name": "Normalize",
      "notes": "## 2) Normalize\n- Decode correctly using MIME charset + transfer-encoding; repair common mojibake only when detected.\n- Sanitize text: strip Unicode format chars (Cf), normalize whitespace/newlines, remove obvious transport wrappers.\n- Choose best “content candidate” (HTML→text or text/plain) and unwrap angle-bracket URLs.\n- Output a single normalized text plus a small metadata block (canonical_url candidate, source, date).\n",
      "parameters": {
        "jsCode": "/**\n * Node1 — Normalization / structural decisions (core_text/core_html -> newsletter_text/newsletter_html)\n *\n * CONTRACT\n * Input:\n *   - core_text (required-ish)\n *   - core_html (optional)\n *\n * Output:\n *   - newsletter_text: best plain-text representation for conversion\n *   - newsletter_html: lightly cleaned HTML (NOT structurally amputated)\n *   - debug fields describing structural decisions (teaser dropped, footer trimmed, etc.)\n *\n * GOALS (per plan)\n *   1) STOP keyword-based container deletion in HTML\n *   2) Reintroduce SAFE teaser/main-content de-dup here (not Node3)\n *   3) Footer trimming is span-aware (trim from anchor to end), consistent method\n *   4) Output both representations for Node2 to choose\n *\n * No external deps; n8n runner-safe.\n */\n\nconst nowIso = new Date().toISOString();\n\n/* ----------------------------- common helpers ----------------------------- */\n\nconst normalizeNewlines = (s) => String(s || \"\").replace(/\\r\\n/g, \"\\n\").replace(/\\r/g, \"\\n\");\n\nconst makeCfRegex = () => {\n  try {\n    return new RegExp(\"\\\\p{Cf}+\", \"gu\"); // format chars (ZWJ/ZWNJ/etc)\n  } catch {\n    return /[\\u200B-\\u200F\\u202A-\\u202E\\u2060-\\u206F\\uFEFF\\u00AD\\u034F]/g;\n  }\n};\nconst RE_CF = makeCfRegex();\n\nconst RE_UNI_SPACE = /[\\u00A0\\u1680\\u2000-\\u200A\\u2007\\u202F\\u205F\\u3000]/g;\n\nconst decodeEntities = (s) =>\n  String(s || \"\")\n    .replace(/&nbsp;/gi, \" \")\n    .replace(/&#160;/g, \" \")\n    .replace(/&amp;/g, \"&\")\n    .replace(/&lt;/g, \"<\")\n    .replace(/&gt;/g, \">\")\n    .replace(/&quot;/g, '\"')\n    .replace(/&#39;/g, \"'\")\n    .replace(/&apos;/g, \"'\");\n\nfunction stripInvisibleTransport(s) {\n  return String(s || \"\")\n    .replace(/\\u0000/g, \"\")\n    .replace(RE_UNI_SPACE, \" \")\n    .replace(RE_CF, \"\")\n    .replace(/\\u00A0/g, \" \");\n}\n\nfunction collapseWhitespacePreserveNewlines(s) {\n  const text = normalizeNewlines(s);\n  const lines = text.split(\"\\n\");\n\n  const out = [];\n  let blankRun = 0;\n\n  for (let line of lines) {\n    line = String(line || \"\")\n      .replace(/[ \\t]+/g, \" \")\n      .trim();\n\n    if (!line) {\n      blankRun += 1;\n      if (blankRun <= 2) out.push(\"\");\n      continue;\n    }\n\n    blankRun = 0;\n    out.push(line);\n  }\n\n  return out.join(\"\\n\").trim();\n}\n\n/* ----------------------------- mojibake guard ---------------------------- */\n\nconst MOJI_SIGNAL = /(?:â[\\u0080-\\u00BF]|Ã[\\u0080-\\u00BF]|Â[\\u0080-\\u00BF]|â€”|â€“|â€™|â€œ|â€\\x9d|â€¢|â€¦|Â )/;\n\nfunction mojiScore(s) {\n  const str = String(s || \"\");\n  if (!str) return 0;\n  const m = str.match(new RegExp(MOJI_SIGNAL.source, \"g\"));\n  const scoreSignals = m ? m.length : 0;\n  const scoreRepl = (str.match(/\\uFFFD/g) || []).length;\n  return scoreSignals + scoreRepl;\n}\n\nfunction fixMojibakeGuarded(s) {\n  const str = String(s || \"\");\n  if (!str) return { text: str, fixed: false, method: \"none\" };\n\n  if (!MOJI_SIGNAL.test(str) && str.indexOf(\"\\uFFFD\") === -1) {\n    return { text: str, fixed: false, method: \"none\" };\n  }\n\n  try {\n    const bytes = Uint8Array.from(str, (c) => c.charCodeAt(0) & 0xff);\n\n    let out = null;\n    if (typeof TextDecoder !== \"undefined\") {\n      out = new TextDecoder(\"utf-8\", { fatal: false }).decode(bytes);\n    } else if (typeof Buffer !== \"undefined\") {\n      out = Buffer.from(bytes).toString(\"utf8\");\n    }\n\n    if (out && out !== str && mojiScore(out) <= mojiScore(str)) {\n      return { text: out, fixed: true, method: \"latin1->utf8\" };\n    }\n  } catch (_) {}\n\n  const out2 = str\n    .replace(/â€™/g, \"’\")\n    .replace(/â€œ/g, \"“\")\n    .replace(/â€\\x9d/g, \"”\")\n    .replace(/â€“/g, \"–\")\n    .replace(/â€”/g, \"—\")\n    .replace(/â€¦/g, \"…\")\n    .replace(/â€¢/g, \"•\")\n    .replace(/Â /g, \" \")\n    .replace(/Â/g, \"\");\n\n  return { text: out2, fixed: out2 !== str, method: out2 !== str ? \"replace\" : \"none\" };\n}\n\n/* ---------------------------- text normalization --------------------------- */\n\n// Drop known transport artifacts (safe)\nfunction dropTransportArtifactLines(text) {\n  const lines = normalizeNewlines(text).split(\"\\n\");\n  const out = [];\n\n  for (const line of lines) {\n    const t = String(line || \"\").trim();\n\n    // n8n / gmail-ish marker lines that are never content\n    if (/^<#m_-?\\d+_>\\s*$/i.test(t)) continue;\n\n    out.push(line);\n  }\n\n  return out.join(\"\\n\");\n}\n\n// Collapse “decorative divider” garbage (incl. mojibake divider runs)\nfunction looksDecorativeDividerLine(line) {\n  const s = String(line || \"\").trim();\n  if (!s) return false;\n\n  // classic separators\n  if (/^[-_=*•·]{6,}$/.test(s)) return true;\n\n  // Mostly non-alnum, long, low variety -> divider junk\n  let alphaNum = 0;\n  let uniq = new Set();\n  for (let i = 0; i < s.length && i < 300; i++) {\n    const c = s[i];\n    uniq.add(c);\n    if ((c >= \"a\" && c <= \"z\") || (c >= \"A\" && c <= \"Z\") || (c >= \"0\" && c <= \"9\")) alphaNum++;\n  }\n  if (s.length >= 24 && alphaNum === 0 && uniq.size <= 8) return true;\n\n  // Unicode-heavy divider (Substack-ish): very low letters, repetitive\n  try {\n    const letters = (s.match(/\\p{L}/gu) || []).length;\n    if (s.length >= 24 && letters === 0 && uniq.size <= 10) return true;\n  } catch {\n    // ignore\n  }\n\n  return false;\n}\n\nfunction collapseDecorativeDividerRuns(text) {\n  const lines = normalizeNewlines(text).split(\"\\n\");\n  const out = [];\n  let lastWasDivider = false;\n\n  for (const line of lines) {\n    const isDiv = looksDecorativeDividerLine(line);\n    if (isDiv) {\n      if (!lastWasDivider) out.push(\"---\"); // normalize run to single separator\n      lastWasDivider = true;\n      continue;\n    }\n    lastWasDivider = false;\n    out.push(line);\n  }\n  return out.join(\"\\n\");\n}\n\n/* ---------------------------- teaser de-dup ---------------------------- */\n\nfunction splitIntoBlocks(text) {\n  const lines = normalizeNewlines(text).split(\"\\n\");\n  const blocks = [];\n  let cur = [];\n\n  for (const line of lines) {\n    if (String(line || \"\").trim() === \"\") {\n      if (cur.length) {\n        blocks.push(cur);\n        cur = [];\n      }\n      continue;\n    }\n    cur.push(line);\n  }\n  if (cur.length) blocks.push(cur);\n  return blocks;\n}\n\nconst TEASER_HEADER_RE = /(view (in (your )?)?browser|view online|read online|open in browser|trouble viewing|having trouble viewing|web version)/i;\n\nfunction stripTeaserNoiseLines(blockText) {\n  const lines = normalizeNewlines(blockText).split(\"\\n\");\n  const kept = [];\n  for (const line of lines) {\n    const t = String(line || \"\").trim();\n    if (!t) continue;\n    if (TEASER_HEADER_RE.test(t)) continue;\n    if (/^(https?:\\/\\/\\S+|www\\.\\S+)\\s*$/i.test(t)) continue; // header links\n    kept.push(line);\n  }\n  return kept.join(\"\\n\").trim();\n}\n\nfunction normalizeForDup(s) {\n  let t = String(s || \"\");\n  // remove URLs (they vary wildly)\n  t = t.replace(/https?:\\/\\/[^\\s<>()]+/gi, \" \");\n  t = t.replace(/www\\.[^\\s<>()]+/gi, \" \");\n  t = t.toLowerCase();\n  // remove punctuation-ish\n  t = t.replace(/[\\u2010-\\u2015]/g, \"-\");\n  t = t.replace(/[^a-z0-9\\s-]+/g, \" \");\n  t = t.replace(/\\s+/g, \" \").trim();\n  return t;\n}\n\nfunction maybeDropTeaser(text) {\n  const blocks = splitIntoBlocks(text);\n  if (blocks.length < 2) {\n    return { text, dropped: false, reason: \"no_blocks\", teaserText: \"\" };\n  }\n\n  const joinedBlocks = blocks.map((b) => b.join(\"\\n\").trim());\n\n  // Try dropping 1..3 top blocks (conservative)\n  const maxDrop = Math.min(3, joinedBlocks.length - 1);\n\n  for (let n = 1; n <= maxDrop; n++) {\n    const before = joinedBlocks.slice(0, n).join(\"\\n\\n\").trim();\n    const after = joinedBlocks.slice(n).join(\"\\n\\n\").trim();\n\n    if (!before || !after) continue;\n\n    const beforeLen = before.length;\n    const afterLen = after.length;\n\n    // Condition: after is substantially longer\n    if (!(afterLen >= beforeLen * 1.8 && afterLen >= 900)) continue;\n\n    // Condition: clear separator OR teaser-ish ending\n    const beforeLastLine = String(joinedBlocks[n - 1].split(\"\\n\").slice(-1)[0] || \"\").trim();\n    const hasSeparator =\n      looksDecorativeDividerLine(beforeLastLine) ||\n      TEASER_HEADER_RE.test(before) ||\n      /\\.\\.\\.\\s*$/.test(before) ||\n      /…\\s*$/.test(before);\n\n    if (!hasSeparator) continue;\n\n    // Duplication signal: normalized \"before\" appears in normalized \"after\"\n    const beforeCore = stripTeaserNoiseLines(before);\n    const beforeCoreNorm = normalizeForDup(beforeCore);\n    const afterNorm = normalizeForDup(after);\n\n    const teaserish = /\\.\\.\\.\\s*$/.test(beforeCore) || /…\\s*$/.test(beforeCore);\n\n    // Require meaningful before core for substring test, unless explicitly teaserish\n    if (beforeCoreNorm.length < 120 && !teaserish) continue;\n\n    // Use a capped prefix for stability\n    const needle = beforeCoreNorm.slice(0, 600);\n    const dup = needle && afterNorm.includes(needle);\n\n    if (!dup && !teaserish) continue;\n\n    // Drop teaser\n    return {\n      text: after.trim(),\n      dropped: true,\n      reason: dup ? \"dup_substring\" : \"teaser_ellipsis\",\n      teaserText: before.trim(),\n      teaserBlocksDropped: n,\n    };\n  }\n\n  return { text, dropped: false, reason: \"no_match\", teaserText: \"\" };\n}\n\n/* ---------------------------- footer trimming ---------------------------- */\n\nconst URL_RE = /(https?:\\/\\/[^\\s<>()]+|www\\.[^\\s<>()]+)/ig;\n\nconst FOOTER_STRONG = [\n  \"unsubscribe\",\n  \"manage preferences\",\n  \"update preferences\",\n  \"email preferences\",\n  \"preferences link\",\n  \"you are receiving\",\n  \"this email was sent\",\n  \"why am i receiving\",\n  \"forward to a friend\",\n  \"email to a friend\",\n  \"view in browser\",\n  \"view in your browser\",\n  \"trouble viewing\",\n  \"having trouble viewing\",\n  \"powered by\",\n  \"feedblitz\",\n  \"substack\",\n  \"mailchimp\",\n  \"beehiiv\",\n  \"convertkit\",\n  \"campaign monitor\",\n  \"constant contact\",\n  \"klaviyo\",\n  \"sendinblue\",\n];\n\nconst FOOTER_WEAK = [\"privacy\", \"contact\", \"archives\", \"subscribe\", \"settings\", \"terms\"];\n\nfunction countAlpha(s) {\n  try {\n    const m = String(s || \"\").match(/\\p{L}/gu);\n    return m ? m.length : 0;\n  } catch {\n    const m = String(s || \"\").match(/[A-Za-z]/g);\n    return m ? m.length : 0;\n  }\n}\n\nfunction lineFeatures(line) {\n  const raw = String(line || \"\");\n  const l = raw.trim();\n  const lower = l.toLowerCase();\n\n  const urls = l.match(URL_RE) || [];\n  const urlCount = urls.length;\n  const urlChars = urls.reduce((a, u) => a + u.length, 0);\n\n  const alpha = countAlpha(l);\n  const total = Math.max(1, l.length);\n  const alphaRatio = alpha / total;\n  const linkDensity = urlChars / total;\n\n  const noUrls = l.replace(URL_RE, \"\").replace(/[\\s\\W_]+/g, \"\");\n  const isUrlOnly = noUrls.length === 0 && urlCount > 0;\n\n  const strongHit = FOOTER_STRONG.some((k) => lower.includes(k));\n  const weakHit = FOOTER_WEAK.some((k) => lower.includes(k));\n\n  const looksAddress =\n    /\\b\\d{1,6}\\b/.test(l) &&\n    /\\b(st|street|ave|avenue|rd|road|suite|ste|po box|p\\.?o\\.?\\s*box|blvd|boulevard|lane|ln|drive|dr)\\b/i.test(l);\n\n  return { lower, alphaRatio, linkDensity, urlCount, isUrlOnly, strongHit, weakHit, looksAddress };\n}\n\nfunction splitBlocksWithSpans(text) {\n  const lines = normalizeNewlines(text).split(\"\\n\");\n  const blocks = [];\n  let cur = [];\n  let start = 0;\n\n  const flush = (endExclusive) => {\n    if (!cur.length) return;\n    blocks.push({ start, end: endExclusive, lines: cur.slice() });\n    cur = [];\n  };\n\n  for (let i = 0; i < lines.length; i++) {\n    const line = lines[i];\n    if (String(line || \"\").trim() === \"\") {\n      flush(i);\n      start = i + 1;\n      continue;\n    }\n    cur.push(line);\n  }\n  flush(lines.length);\n\n  return { lines, blocks };\n}\n\nfunction blockFooterScore(blockLines) {\n  let score = 0;\n  for (const line of blockLines) {\n    const l = String(line || \"\").trim();\n    if (!l) continue;\n\n    const f = lineFeatures(l);\n\n    if (f.strongHit) score += 5;\n    if (f.weakHit) score += 2;\n    if (f.looksAddress) score += 3;\n\n    if (f.isUrlOnly) score += 2;\n    if (f.urlCount > 0 && f.alphaRatio < 0.22) score += 1;\n    if (f.linkDensity > 0.55) score += 2;\n\n    // short nav-ish lines\n    if (l.length <= 70 && (f.strongHit || f.weakHit || f.isUrlOnly)) score += 1;\n  }\n  return score;\n}\n\nfunction findFooterAnchorLineIndex(lines) {\n  const TAIL_LINES = 140;\n  const start = Math.max(0, lines.length - TAIL_LINES);\n\n  let best = null; // { idx, score }\n  for (let i = start; i < lines.length; i++) {\n    const t = String(lines[i] || \"\").trim();\n    if (!t) continue;\n\n    const f = lineFeatures(t);\n\n    // anchor score (line-based)\n    let s = 0;\n    if (f.strongHit) s += 6;\n    if (f.weakHit) s += 2;\n    if (f.looksAddress) s += 4;\n\n    if (f.isUrlOnly) s += 2;\n    if (f.urlCount > 0 && f.alphaRatio < 0.20) s += 2;\n    if (f.linkDensity > 0.60) s += 2;\n\n    // require a meaningful anchor\n    if (s < 6) continue;\n\n    // prefer earlier anchor (trim more), but keep strongest if tie-ish\n    if (!best || i < best.idx || (i === best.idx && s > best.score)) {\n      best = { idx: i, score: s };\n    }\n  }\n\n  return best;\n}\n\nfunction trimFooterSpanAware(text) {\n  const original = text;\n  const { lines, blocks } = splitBlocksWithSpans(text);\n\n  if (!lines.length) {\n    return {\n      text: \"\",\n      trimmed: false,\n      reason: \"empty\",\n      anchor: null,\n      blocksDropped: 0,\n      removedChars: 0,\n    };\n  }\n\n  // Step 1: drop trailing footer-ish blocks (consistent, block-based)\n  let endBlock = blocks.length - 1;\n  let blocksDropped = 0;\n\n  while (endBlock >= 0) {\n    const score = blockFooterScore(blocks[endBlock].lines);\n\n    // Conservative threshold: only drop clearly footer-ish blocks\n    if (score >= 8) {\n      endBlock--;\n      blocksDropped++;\n      continue;\n    }\n    break;\n  }\n\n  let keptLines = lines.slice(0);\n  if (blocks.length && endBlock < blocks.length - 1) {\n    const cutLine = blocks[endBlock] ? blocks[endBlock].end : 0;\n    keptLines = lines.slice(0, cutLine);\n  }\n\n  // Step 2: anchor-based trim within the remaining tail (span-aware)\n  const anchor = findFooterAnchorLineIndex(keptLines);\n  let anchorTrimmed = false;\n\n  if (anchor && anchor.idx >= Math.floor(keptLines.length * 0.55)) {\n    // only trim from anchor if it’s in the last ~45% of the email\n    keptLines = keptLines.slice(0, anchor.idx);\n    anchorTrimmed = true;\n  }\n\n  let out = keptLines.join(\"\\n\").trim();\n\n  // Safety: don’t nuke content accidentally\n  const origLen = String(original || \"\").trim().length;\n  const outLen = out.length;\n\n  const tooSmall =\n    (origLen >= 1200 && outLen < 300) ||\n    (origLen >= 2000 && outLen < origLen * 0.25) ||\n    (origLen >= 4000 && outLen < origLen * 0.18);\n\n  if (tooSmall) {\n    return {\n      text: String(original || \"\").trim(),\n      trimmed: false,\n      reason: \"safety_abort\",\n      anchor,\n      blocksDropped,\n      removedChars: 0,\n      anchorTrimmed,\n    };\n  }\n\n  const removedChars = Math.max(0, origLen - outLen);\n\n  return {\n    text: out,\n    trimmed: blocksDropped > 0 || anchorTrimmed,\n    reason: anchorTrimmed ? \"anchor_trim\" : blocksDropped > 0 ? \"block_trim\" : \"none\",\n    anchor,\n    blocksDropped,\n    removedChars,\n    anchorTrimmed,\n  };\n}\n\n/* ------------------------------ HTML cleanup ------------------------------ */\n/**\n * IMPORTANT: no keyword-based container deletion.\n * Only safe removals:\n *   - scripts/styles\n *   - tracking pixels (1x1 / display:none / width:0 height:0 / opacity:0)\n *   - obvious hidden preheaders (display:none etc; small; near top)\n */\nfunction cleanHtmlLight(html) {\n  if (!html) return null;\n\n  let h = String(html);\n\n  // hard cap (defensive)\n  if (h.length > 2_000_000) h = h.slice(0, 2_000_000);\n\n  // strip scripts/styles\n  h = h.replace(/<script[\\s\\S]*?<\\/script>/gi, \"\");\n  h = h.replace(/<style[\\s\\S]*?<\\/style>/gi, \"\");\n\n  // tracking pixel img tags (keep conservative)\n  h = h.replace(\n    /<img[^>]+(?:width=[\"']?\\s*(?:0|1)\\s*(?:px)?[\"']?|height=[\"']?\\s*(?:0|1)\\s*(?:px)?[\"']?|style=[\"'][^\"']*(?:display\\s*:\\s*none|opacity\\s*:\\s*0|visibility\\s*:\\s*hidden|width\\s*:\\s*(?:0|1)px|height\\s*:\\s*(?:0|1)px)[^\"']*[\"'])[^>]*>/gi,\n    \"\"\n  );\n\n  // obvious hidden preheaders (small, near top)\n  const TOP_SCAN = 60_000;\n  const top = h.slice(0, TOP_SCAN);\n  const rest = h.slice(TOP_SCAN);\n\n  const hiddenPreheaderRe =\n    /<(div|span|p|td)[^>]*style=[\"'][^\"']*(?:display\\s*:\\s*none|visibility\\s*:\\s*hidden|opacity\\s*:\\s*0|max-height\\s*:\\s*0|mso-hide\\s*:\\s*all)[^\"']*[\"'][^>]*>[\\s\\S]{0,4000}?<\\/\\1>/gi;\n\n  const topCleaned = top.replace(hiddenPreheaderRe, \"\");\n\n  h = topCleaned + rest;\n\n  // normalize transport invisibles (does NOT restructure)\n  const moji = fixMojibakeGuarded(h);\n  h = stripInvisibleTransport(moji.text);\n  h = normalizeNewlines(h).trim();\n\n  return { html: h || null, mojibakeFixed: !!moji.fixed, mojibakeMethod: moji.method };\n}\n\n/* ---------------------------------- inputs ---------------------------------- */\n\nlet coreText = String($json.core_text || \"\");\nlet coreHtml = $json.core_html || null;\n\nconst debug_in_core_text_len = coreText.length;\nconst debug_in_core_html_len = coreHtml ? String(coreHtml).length : 0;\n\n/* ----------------------------- build newsletter_text ----------------------------- */\n\nlet text = coreText;\n\n// baseline cleanup (safe)\nconst mojiText = fixMojibakeGuarded(text);\ntext = mojiText.text;\n\ntext = decodeEntities(text);\ntext = stripInvisibleTransport(text);\ntext = dropTransportArtifactLines(text);\ntext = collapseDecorativeDividerRuns(text);\ntext = collapseWhitespacePreserveNewlines(text);\n\n// teaser de-dup (safe + conservative)\nconst teaser = maybeDropTeaser(text);\nlet newsletter_text = teaser.text;\n\n// footer trimming (span-aware)\nconst footer = trimFooterSpanAware(newsletter_text);\nnewsletter_text = footer.text;\n\n// final normalization pass (keeps stable)\nnewsletter_text = collapseWhitespacePreserveNewlines(newsletter_text);\n\n/* ----------------------------- build newsletter_html ----------------------------- */\n\nlet newsletter_html = null;\nlet debug_html_mojibake_fixed = false;\nlet debug_html_mojibake_method = \"none\";\n\nif (coreHtml && String(coreHtml).trim()) {\n  const cleaned = cleanHtmlLight(coreHtml);\n  if (cleaned) {\n    newsletter_html = cleaned.html;\n    debug_html_mojibake_fixed = cleaned.mojibakeFixed;\n    debug_html_mojibake_method = cleaned.mojibakeMethod;\n  }\n}\n\n/* ---------------------------------- debug ---------------------------------- */\n\nconst debug_out_newsletter_text_len = newsletter_text.length;\nconst debug_out_newsletter_html_len = newsletter_html ? newsletter_html.length : 0;\n\nconst capDebugText = (s, max = 4000) => {\n  const t = String(s || \"\");\n  return t.length > max ? t.slice(0, max) + \"\\n…[truncated]\" : t;\n};\n\nreturn [\n  {\n    json: {\n      ...$json,\n\n      // outputs\n      newsletter_text,\n      newsletter_html,\n\n      // timestamps\n      node1_at: nowIso,\n\n      // debug / metrics\n      debug_node1_in_core_text_len: debug_in_core_text_len,\n      debug_node1_in_core_html_len: debug_in_core_html_len,\n      debug_node1_out_newsletter_text_len: debug_out_newsletter_text_len,\n      debug_node1_out_newsletter_html_len: debug_out_newsletter_html_len,\n\n      // teaser debug\n      debug_teaser_dropped: !!teaser.dropped,\n      debug_teaser_reason: teaser.reason,\n      debug_teaser_blocks_dropped: teaser.teaserBlocksDropped || 0,\n      debug_teaser_text: teaser.dropped ? capDebugText(teaser.teaserText) : \"\",\n\n      // footer debug\n      debug_footer_trimmed: !!footer.trimmed,\n      debug_footer_reason: footer.reason,\n      debug_footer_blocks_dropped: footer.blocksDropped || 0,\n      debug_footer_anchor_line: footer.anchor ? footer.anchor.idx : -1,\n      debug_footer_anchor_score: footer.anchor ? footer.anchor.score : 0,\n      debug_footer_anchor_trimmed: !!footer.anchorTrimmed,\n      debug_footer_removed_chars: footer.removedChars || 0,\n\n      // mojibake debug\n      debug_node1_text_mojibake_fixed: !!mojiText.fixed,\n      debug_node1_text_mojibake_method: mojiText.method,\n      debug_node1_html_mojibake_fixed: debug_html_mojibake_fixed,\n      debug_node1_html_mojibake_method: debug_html_mojibake_method,\n    },\n  },\n];\n"
      },
      "position": [
        576,
        -576
      ],
      "type": "n8n-nodes-base.code",
      "typeVersion": 2
    },
    {
      "credentials": {
        "openAiApi": {
          "id": "CPyLLVga3nhZAwjZ",
          "name": "OpenAi account"
        }
      },
      "id": "ba9718a6-6d18-4ef5-a351-4bc752674d91",
      "name": "Message a model",
      "parameters": {
        "builtInTools": {},
        "modelId": {
          "__rl": true,
          "cachedResultName": "GPT-5-NANO",
          "mode": "list",
          "value": "gpt-5-nano"
        },
        "options": {},
        "responses": {
          "values": [
            {
              "content": "You are a precise metadata extraction engine for a personal knowledge base.\nReturn ONLY valid JSON that matches the requested schema.\nDo not include markdown, comments, or any extra text.\nBe conservative: if unsure, lower confidence and use \"other\" for primary topic.",
              "role": "system"
            },
            {
              "content": "={{ $json.prompt }}"
            }
          ]
        }
      },
      "position": [
        2576,
        -272
      ],
      "type": "@n8n/n8n-nodes-langchain.openAi",
      "typeVersion": 2.1
    },
    {
      "id": "eb51b9e0-00e7-4fb0-841e-989087f1fe8c",
      "name": "Topics = {}",
      "parameters": {
        "assignments": {
          "assignments": [
            {
              "id": "3cfcd2ec-a545-4735-89d2-c443310bed01",
              "name": "topics",
              "type": "string",
              "value": "productivity, product, entrepreneurship, consulting, marketing, finance, parenting, engineering, ai, home, leadership, cookbook, fitness, communication, other, health, travel"
            }
          ]
        },
        "includeOtherFields": true,
        "options": {}
      },
      "position": [
        1904,
        -272
      ],
      "type": "n8n-nodes-base.set",
      "typeVersion": 3.4
    },
    {
      "id": "4da5cf51-2d6b-45a3-a9c8-e13f63bad821",
      "name": "Create Whole Text Prompt",
      "parameters": {
        "jsCode": "// Inputs expected on $json:\n// title, author, content_type, topics, clean_text\n// No normalization / cleanup performed here.\n\n// HARD RULE: must have non-empty clean_text or fail (do not enrich)\nconst text = ($json.clean_text ?? '').toString();\nif (!text.trim()) {\n  throw new Error('Tier-1 prompt: clean_text is empty/null — abort enrichment');\n}\n\nconst TITLE = ($json.title ?? '').toString();\nconst AUTHOR = ($json.author ?? '').toString();\nconst CONTENT_TYPE = ($json.content_type ?? 'other').toString();\n\nlet TOPIC_LIST = $json.topics ?? '[\"other\"]';\nif (typeof TOPIC_LIST !== 'string') {\n  TOPIC_LIST = JSON.stringify(TOPIC_LIST);\n}\n\n// RULE: For short texts (< 4000 chars in your IF), send whole thing.\n// Still keep a hard cap in case IF threshold changes or upstream surprises.\nconst MAX_TOTAL_CHARS_SENT = 6000;\nconst full = text.length > MAX_TOTAL_CHARS_SENT ? text.slice(0, MAX_TOTAL_CHARS_SENT) : text;\n\nconst prompt =\n`TASK\nExtract Tier-1 metadata for the item below. I will store your JSON output in a database for retrieval.\n\nCONTROLLED TOPICS (primary topic must be exactly one of these, else \"other\"):\n${TOPIC_LIST}\n\nRULES\n1) Choose \"topic_primary\" from the controlled list only. If none fits well, use \"other\".\n2) Choose \"topic_secondary\" as a short freeform label (2–5 words), lowercase, no punctuation. It should be more specific than topic_primary.\n3) Extract 5–12 keywords as short noun phrases, lowercase, no punctuation, no stopwords, no duplicates.\n4) Write \"gist\" as ONE sentence (max 25 words) describing the core idea.\n6) Provide confidence scores 0.0–1.0 for topic_primary and topic_secondary.\n\nOUTPUT JSON SCHEMA (return exactly these keys)\n{\n  \"topic_primary\": \"string\",\n  \"topic_primary_confidence\": number,\n  \"topic_secondary\": \"string\",\n  \"topic_secondary_confidence\": number,\n  \"keywords\": [\"string\", ...],\n  \"gist\": \"string\",\n  \"flags\": {\n    \"boilerplate_heavy\": boolean,\n    \"low_signal\": boolean\n  }\n}\n\nITEM METADATA\ntitle: ${TITLE}\nauthor: ${AUTHOR}\ncontent_type: ${CONTENT_TYPE}\n\nITEM TEXT (FULL)\n${full}\n`;\n\nreturn [{ json: { ...$json, prompt, prompt_mode: \"whole\" } }];\n"
      },
      "position": [
        2352,
        -368
      ],
      "type": "n8n-nodes-base.code",
      "typeVersion": 2
    },
    {
      "id": "16117688-d107-4f1d-81d0-048fb7619e92",
      "name": "Create Sampled Prompt",
      "parameters": {
        "jsCode": "// Inputs expected on $json:\n// title, author, content_type, topics, clean_text\n// Only allowed modification: head / mid1 / mid2 / tail sampling.\n\nconst TITLE = ($json.title ?? '').toString();\nconst AUTHOR = ($json.author ?? '').toString();\nconst CONTENT_TYPE = ($json.content_type ?? 'other').toString();\n\nlet TOPIC_LIST = $json.topics ?? '[\"other\"]';\nif (typeof TOPIC_LIST !== 'string') {\n  TOPIC_LIST = JSON.stringify(TOPIC_LIST);\n}\n\n// HARD RULE: must have non-empty clean_text or fail (do not enrich)\nconst text = ($json.clean_text ?? '').toString();\nif (!text.trim()) {\n  throw new Error('Tier-1 prompt: clean_text is empty/null — abort enrichment');\n}\n\nconst n = text.length;\n\n// Caps (Tier 1)\nconst MAX_TOTAL_CHARS_SENT = 6000;\nconst HEAD_CHARS = 3000;\nconst TAIL_CHARS = 1200;\nconst MID1_CHARS = 600;\nconst MID2_CHARS = 600;\n\n// Sampling helpers\nfunction windowAt(text, centerIndex, windowLen) {\n  const half = Math.floor(windowLen / 2);\n  let start = Math.max(0, centerIndex - half);\n  let end = Math.min(text.length, start + windowLen);\n  start = Math.max(0, end - windowLen);\n  return text.slice(start, end);\n}\n\nlet head = text.slice(0, Math.min(HEAD_CHARS, n));\nlet tail = n <= TAIL_CHARS ? text : text.slice(n - TAIL_CHARS);\nlet mid1 = windowAt(text, Math.floor(n * 0.33), MID1_CHARS);\nlet mid2 = windowAt(text, Math.floor(n * 0.66), MID2_CHARS);\n\n// Build prompt (exact format you gave)\nfunction buildPrompt(h, m1, m2, t) {\n  return (\n`TASK\nExtract Tier-1 metadata for the item below. I will store your JSON output in a database for retrieval.\n\nCONTROLLED TOPICS (primary topic must be exactly one of these, else \"other\"):\n${TOPIC_LIST}\n\nRULES\n1) Choose \"topic_primary\" from the controlled list only. If none fits well, use \"other\".\n2) Choose \"topic_secondary\" as a short freeform label (2–5 words), lowercase, no punctuation. It should be more specific than topic_primary.\n   - Examples: \"sibling conflict\", \"ai trust\", \"newsletter strategy\", \"decision hygiene\"\n3) Extract 5–12 keywords as short noun phrases, lowercase, no punctuation, no stopwords, no duplicates.\n4) Write \"gist\" as ONE sentence (max 25 words) describing the core idea.\n6) Provide confidence scores 0.0–1.0 for topic_primary and topic_secondary.\n\nOUTPUT JSON SCHEMA (return exactly these keys)\n{\n  \"topic_primary\": \"string\",\n  \"topic_primary_confidence\": number,\n  \"topic_secondary\": \"string\",\n  \"topic_secondary_confidence\": number,\n  \"keywords\": [\"string\", ...],\n  \"gist\": \"string\",\n  \"flags\": {\n    \"boilerplate_heavy\": boolean,\n    \"low_signal\": boolean\n  }\n}\n\nITEM METADATA\ntitle: ${TITLE}\nauthor: ${AUTHOR}\ncontent_type: ${CONTENT_TYPE}\n\nITEM TEXT (SAMPLED)\nhead:\n${h}\n\nmid_1:\n${m1}\n\nmid_2:\n${m2}\n\ntail:\n${t}`\n  );\n}\n\n// Hard-cap enforcement: trim tail, then mid2, then mid1. (No other modifications)\nlet prompt = buildPrompt(head, mid1, mid2, tail);\n\nfunction rebuild() { prompt = buildPrompt(head, mid1, mid2, tail); }\nfunction trimRight(s, charsToRemove) {\n  if (charsToRemove <= 0) return s;\n  return s.slice(0, Math.max(0, s.length - charsToRemove));\n}\n\nif (prompt.length > MAX_TOTAL_CHARS_SENT) {\n  let over = prompt.length - MAX_TOTAL_CHARS_SENT;\n  const cut = Math.min(over, tail.length);\n  tail = trimRight(tail, cut);\n  rebuild();\n}\nif (prompt.length > MAX_TOTAL_CHARS_SENT) {\n  let over = prompt.length - MAX_TOTAL_CHARS_SENT;\n  const cut = Math.min(over, mid2.length);\n  mid2 = trimRight(mid2, cut);\n  rebuild();\n}\nif (prompt.length > MAX_TOTAL_CHARS_SENT) {\n  let over = prompt.length - MAX_TOTAL_CHARS_SENT;\n  const cut = Math.min(over, mid1.length);\n  mid1 = trimRight(mid1, cut);\n  rebuild();\n}\n// If still over (rare due to fixed template), last resort: hard slice prompt itself.\nif (prompt.length > MAX_TOTAL_CHARS_SENT) {\n  prompt = prompt.slice(0, MAX_TOTAL_CHARS_SENT);\n}\n\nreturn [{\n  json: {\n    ...$json,\n    prompt,\n    prompt_mode: \"sample\",\n    text_head: head,\n    text_mid1: mid1,\n    text_mid2: mid2,\n    text_tail: tail\n  }\n}];\n"
      },
      "position": [
        2352,
        -176
      ],
      "type": "n8n-nodes-base.code",
      "typeVersion": 2
    },
    {
      "id": "4a888159-69d8-45c1-a32b-9db65d41d383",
      "name": "If TEXT < 4000",
      "parameters": {
        "conditions": {
          "combinator": "and",
          "conditions": [
            {
              "id": "f2d44d52-7191-4f3f-a162-e23056974265",
              "leftValue": "={{ $json.text_len }}",
              "operator": {
                "operation": "lt",
                "type": "number"
              },
              "rightValue": 4000
            }
          ],
          "options": {
            "caseSensitive": true,
            "leftValue": "",
            "typeValidation": "strict",
            "version": 3
          }
        },
        "options": {}
      },
      "position": [
        2128,
        -272
      ],
      "type": "n8n-nodes-base.if",
      "typeVersion": 2.3
    },
    {
      "id": "8a665982-0789-44a2-afc0-7aaddfcc8095",
      "name": "Merge",
      "parameters": {
        "combineBy": "combineByPosition",
        "mode": "combine",
        "options": {}
      },
      "position": [
        2944,
        -480
      ],
      "type": "n8n-nodes-base.merge",
      "typeVersion": 3.2
    },
    {
      "credentials": {
        "postgres": {
          "id": "YICLDaLPmdddPcoA",
          "name": "Postgres account"
        }
      },
      "id": "3a7cd01b-4f5a-420f-9215-3522d27e6ab8",
      "name": "Postgres Update Extract",
      "parameters": {
        "operation": "executeQuery",
        "options": {},
        "query": "={{$json.sql}}"
      },
      "position": [
        3600,
        -480
      ],
      "type": "n8n-nodes-base.postgres",
      "typeVersion": 2
    },
    {
      "id": "0ae5a1f1-fc06-445c-b460-7447a1632dba",
      "name": "Compose Reply Text",
      "parameters": {
        "jsCode": "function s(v) { return (v ?? '').toString().trim(); }\n\nconst entryId = s($json.entry_id);\nconst author = s($json.author);\nconst title = s($json.title);\n\n// your current logic\nconst textLen = Number.isFinite(Number($json.clean_text?.length))\n  ? Number($json.clean_text.length)\n  : 0;\n\nconst topicPrimary = s($json.topic_primary);\nconst topicSecondary = s($json.topic_secondary);\nconst gist = s($json.gist);\n\n// flags_json may be object or string\nlet flags = $json.flags_json;\nif (typeof flags === 'string' && flags.trim()) {\n  try { flags = JSON.parse(flags); } catch { flags = {}; }\n}\nif (!flags || typeof flags !== 'object') flags = {};\n\nconst boilerplate = flags.boilerplate_heavy === true;\nconst lowSignal = flags.low_signal === true;\n\nconst lines = [];\n\n// Head emoji + author + id\nlines.push(`🗣️ ${author || 'unknown'}${entryId ? ` (#${entryId})` : ''}`);\n\n// Title\nif (title) lines.push(`📰 ${title}`);\n\n// Length\nlines.push(`📏 ${textLen.toLocaleString()} chars`);\n\n// Topics\nif (topicPrimary && topicSecondary) lines.push(`🏷️ ${topicPrimary} → ${topicSecondary}`);\nelse if (topicPrimary) lines.push(`🏷️ ${topicPrimary}`);\n\n// Gist\nif (gist) lines.push(`\\n_${gist}_`);\n\n// Flags\nconst flagBits = [];\nif (boilerplate) flagBits.push('⚠️ boilerplate-heavy');\nif (lowSignal) flagBits.push('🟡 low-signal');\nif (flagBits.length) lines.push(`\\n${flagBits.join(' · ')}`);\n\nlet telegram_message = lines.join('\\n');\n\n// hard cap for Telegram\nconst MAX = 4000;\nif (telegram_message.length > MAX) telegram_message = telegram_message.slice(0, MAX - 1) + '…';\n\nreturn [{ json: { ...$json, telegram_message } }];\n"
      },
      "position": [
        3824,
        -480
      ],
      "type": "n8n-nodes-base.code",
      "typeVersion": 2
    },
    {
      "credentials": {
        "telegramApi": {
          "id": "4svco1wrzYsvoLNB",
          "name": "Telegram account"
        }
      },
      "id": "ca4cd3a5-b699-45ff-b29e-10f6c352ea16",
      "name": "Send a text message",
      "parameters": {
        "additionalFields": {
          "appendAttribution": false
        },
        "chatId": "1509032341",
        "text": "={{ $json.telegram_message }}"
      },
      "position": [
        4048,
        -480
      ],
      "type": "n8n-nodes-base.telegram",
      "typeVersion": 1.2,
      "webhookId": "6f123209-5952-4fa4-b096-5a03d6e8d0f2"
    },
    {
      "id": "2b306dd2-9dfb-4d42-9302-0258fd2d5ce0",
      "name": "Compute Retrieval Excerpt + Quality (email clean_text)",
      "parameters": {
        "jsCode": "// Email WP1: compute retrieval excerpt + quality signals.\n// HARD RULE: clean_text must be non-empty, else fail.\n// Output: $json.retrieval + $json.metadata_patch\n\nconst cfg = $json.config?.qualityThresholds || {};\nconst TH = {\n  excerpt_max_chars: cfg.excerpt_max_chars ?? 320,\n  low_signal: {\n    min_words: cfg.low_signal?.min_words ?? 35,\n    min_chars: cfg.low_signal?.min_chars ?? 220,\n  },\n  boilerplate: {\n    link_ratio_high: cfg.boilerplate?.link_ratio_high ?? 0.18,\n    link_count_high: cfg.boilerplate?.link_count_high ?? 25,\n  },\n  extraction_incomplete: {\n    min_extracted_chars_to_consider: cfg.extraction_incomplete?.min_extracted_chars_to_consider ?? 800,\n    clean_vs_extracted_ratio_low: cfg.extraction_incomplete?.clean_vs_extracted_ratio_low ?? 0.25,\n  },\n};\n\nfunction normWS(s) {\n  return String(s || '').replace(/\\s+/g, ' ').trim();\n}\n\nfunction trimUrl(u) {\n  let url = String(u || '');\n  while (/[)\\],.?!:;\"'»]+$/.test(url)) url = url.replace(/[)\\],.?!:;\"'»]+$/, '');\n  return url;\n}\n\nfunction linkCountFromText(text) {\n  const matches = String(text || '').match(/https?:\\/\\/[^\\s<>()]+/gi) || [];\n  return matches.map(trimUrl).filter(Boolean).length;\n}\n\nfunction getDomain(rawUrl) {\n  const u = String(rawUrl || '').trim();\n  const m = u.match(/^https?:\\/\\/([^\\/?#]+)/i);\n  if (!m) return null;\n  let host = m[1].toLowerCase();\n  host = host.replace(/:\\d+$/, '');\n  host = host.replace(/^www\\./, '');\n  return host || null;\n}\n\nfunction buildExcerpt(raw, maxChars) {\n  const s = normWS(raw);\n  if (!s) return '';\n  const noUrls = normWS(s.replace(/https?:\\/\\/[^\\s<>()]+/gi, ''));\n  const base = (noUrls.length >= 50) ? noUrls : s;\n  if (base.length <= maxChars) return base;\n  const cut = base.slice(0, maxChars + 1);\n  let idx = cut.lastIndexOf(' ');\n  if (idx < Math.floor(maxChars * 0.6)) idx = maxChars;\n  return base.slice(0, idx).replace(/\\s+$/g, '') + '…';\n}\n\nfunction clamp01(x) {\n  if (x < 0) return 0;\n  if (x > 1) return 1;\n  return x;\n}\n\n// HARD RULE: must have clean_text\nconst clean_text = String($json.clean_text ?? '');\nconst base_text = normWS(clean_text);\n\nif (!base_text) {\n  throw new Error('Fail-fast: clean_text is empty after trim (do not enrich)');\n}\n\nconst clean_word_count = base_text.split(/\\s+/).filter(Boolean).length;\nconst clean_char_count = base_text.length;\n\nconst extracted_text = String($json.extracted_text ?? ''); // usually empty in email flow\nconst extracted_char_count = extracted_text.length;\n\nconst link_count = linkCountFromText(base_text);\nconst link_ratio = link_count / Math.max(1, clean_word_count);\n\nconst low_signal =\n  (clean_word_count < TH.low_signal.min_words) ||\n  (clean_char_count < TH.low_signal.min_chars);\n\nconst boilerplate_heavy =\n  (link_ratio > TH.boilerplate.link_ratio_high) ||\n  (link_count >= TH.boilerplate.link_count_high);\n\nconst extraction_incomplete =\n  (extracted_char_count >= TH.extraction_incomplete.min_extracted_chars_to_consider) &&\n  (clean_char_count / Math.max(1, extracted_char_count) < TH.extraction_incomplete.clean_vs_extracted_ratio_low);\n\nconst signal =\n  0.6 * Math.min(1, clean_word_count / 120) +\n  0.4 * Math.min(1, clean_char_count / 1200);\n\nconst penalty =\n  (boilerplate_heavy ? 0.25 : 0) +\n  (low_signal ? 0.35 : 0) +\n  (extraction_incomplete ? 0.15 : 0);\n\nconst quality_score = clamp01(signal - penalty);\n\nconst source_domain = getDomain($json.url_canonical || $json.url || null);\nconst excerpt = buildExcerpt(base_text, TH.excerpt_max_chars);\n\nconst retrieval = {\n  version: 'v1',\n  excerpt,\n  source_domain,\n  quality: {\n    clean_word_count,\n    clean_char_count,\n    extracted_char_count,\n    link_count,\n    link_ratio,\n    boilerplate_heavy,\n    low_signal,\n    extraction_incomplete,\n    quality_score,\n  },\n};\n\nreturn [{\n  json: {\n    ...$json,\n    retrieval,\n    metadata_patch: { retrieval },\n  }\n}];\n"
      },
      "position": [
        1248,
        -480
      ],
      "type": "n8n-nodes-base.code",
      "typeVersion": 2
    },
    {
      "credentials": {
        "postgres": {
          "id": "YICLDaLPmdddPcoA",
          "name": "Postgres account"
        }
      },
      "id": "afaaf565-d08b-4822-8ca3-4ed3a6546969",
      "name": "Postgres INSERT",
      "parameters": {
        "operation": "executeQuery",
        "options": {},
        "query": "={{$json.sql}}"
      },
      "position": [
        1696,
        -480
      ],
      "type": "n8n-nodes-base.postgres",
      "typeVersion": 2
    },
    {
      "id": "595a102d-251f-4248-9cfd-151300fde930",
      "name": "Call 'PKM — Retrieval Config'",
      "parameters": {
        "options": {
          "waitForSubWorkflow": true
        },
        "workflowId": {
          "__rl": true,
          "cachedResultName": "PKM — Retrieval Config",
          "cachedResultUrl": "/workflow/Ue-cGR6_WP4Fe5C19tA28",
          "mode": "list",
          "value": "Ue-cGR6_WP4Fe5C19tA28"
        },
        "workflowInputs": {
          "attemptToConvertTypes": false,
          "convertFieldsToString": true,
          "mappingMode": "defineBelow",
          "matchingColumns": [],
          "schema": [],
          "value": {}
        }
      },
      "position": [
        -96,
        -480
      ],
      "type": "n8n-nodes-base.executeWorkflow",
      "typeVersion": 1.3
    },
    {
      "id": "c4848348-bcd7-42b5-80d4-5b59e0152a45",
      "name": "Build SQL INSERT",
      "parameters": {
        "jsCode": "function esc(s) {\n  return String(s).replace(/\\\\/g, '\\\\\\\\').replace(/'/g, \"''\");\n}\nfunction lit(v) {\n  return (v === null || v === undefined) ? 'NULL' : `'${esc(v)}'`;\n}\nfunction jsonbLit(obj) {\n  if (obj === null || obj === undefined) return 'NULL';\n  const s = JSON.stringify(obj).replace(/\\\\/g, '\\\\\\\\').replace(/'/g, \"''\");\n  return `'${s}'::jsonb`;\n}\nfunction intLit(v) {\n  if (v === null || v === undefined) return 'NULL';\n  const n = Number(v);\n  if (!Number.isFinite(n)) return 'NULL';\n  return String(Math.trunc(n));\n}\nfunction numLit(v) {\n  if (v === null || v === undefined) return 'NULL';\n  const n = Number(v);\n  if (!Number.isFinite(n)) return 'NULL';\n  return String(n);\n}\nfunction boolLit(v) {\n  if (v === null || v === undefined) return 'NULL';\n  return v ? 'true' : 'false';\n}\n\n// core fields from your normalize node\nconst source = 'email';\nconst intent = $json.intent ?? 'archive';\nconst content_type = $json.content_type ?? null;\n\nconst title = $json.title ?? null;\nconst author = $json.author ?? null;\n\nconst capture_text = $json.capture_text ?? '';\nconst clean_text = $json.clean_text ?? '';\n\nconst url = $json.url ?? null;\nconst url_canonical = $json.url_canonical ?? null;\n\nconst external_ref = $json.external_ref ?? null;\n\n// WP1 metadata patch\nconst metadata_patch = $json.metadata_patch ?? ($json.retrieval ? { retrieval: $json.retrieval } : null);\n\n// WP2 promoted retrieval columns\nconst r = $json.retrieval ?? metadata_patch?.retrieval ?? null;\nconst q = r?.quality ?? {};\n\nconst retrieval_excerpt = r?.excerpt ?? null;\nconst retrieval_version = r?.version ?? null;\nconst source_domain = r?.source_domain ?? null;\n\nconst clean_word_count = q.clean_word_count ?? null;\nconst clean_char_count = q.clean_char_count ?? null;\nconst extracted_char_count = q.extracted_char_count ?? null;\n\nconst link_count = q.link_count ?? null;\nconst link_ratio = q.link_ratio ?? null;\n\nconst boilerplate_heavy = q.boilerplate_heavy ?? null;\nconst low_signal = q.low_signal ?? null;\nconst extraction_incomplete = q.extraction_incomplete ?? null;\n\nconst quality_score = q.quality_score ?? null;\n\nconst sql = `\nINSERT INTO pkm.entries (\n  created_at,\n  source,\n  intent,\n  content_type,\n  title,\n  author,\n  capture_text,\n  clean_text,\n  url,\n  url_canonical,\n  external_ref,\n  metadata,\n  enrichment_status,\n\n  -- WP2 promoted retrieval columns\n  retrieval_excerpt,\n  retrieval_version,\n  source_domain,\n  clean_word_count,\n  clean_char_count,\n  extracted_char_count,\n  link_count,\n  link_ratio,\n  boilerplate_heavy,\n  low_signal,\n  extraction_incomplete,\n  quality_score\n)\nVALUES (\n  now(),\n  ${lit(source)}::text,\n  ${lit(intent)}::text,\n  ${lit(content_type)}::text,\n  ${lit(title)}::text,\n  ${lit(author)}::text,\n  ${lit(capture_text)}::text,\n  ${lit(clean_text)}::text,\n  ${lit(url)}::text,\n  ${lit(url_canonical)}::text,\n  ${jsonbLit(external_ref)},\n  ${jsonbLit(metadata_patch)},\n  'pending',\n\n  ${lit(retrieval_excerpt)}::text,\n  ${lit(retrieval_version)}::text,\n  ${lit(source_domain)}::text,\n  ${intLit(clean_word_count)}::int,\n  ${intLit(clean_char_count)}::int,\n  ${intLit(extracted_char_count)}::int,\n  ${intLit(link_count)}::int,\n  ${numLit(link_ratio)}::real,\n  ${boolLit(boilerplate_heavy)}::boolean,\n  ${boolLit(low_signal)}::boolean,\n  ${boolLit(extraction_incomplete)}::boolean,\n  ${numLit(quality_score)}::real\n)\nRETURNING\n  entry_id,\n  id,\n  created_at,\n  source,\n  intent,\n  content_type,\n  title,\n  author,\n  clean_text,\n  metadata,\n  COALESCE(char_length(clean_text), 0) AS clean_len;\n`.trim();\n\nreturn [{ json: { ...$json, sql } }];\n"
      },
      "position": [
        1472,
        -480
      ],
      "type": "n8n-nodes-base.code",
      "typeVersion": 2
    },
    {
      "id": "5cd4f138-20ac-4c36-867b-e0f618fd3149",
      "name": "Parse T1",
      "parameters": {
        "jsCode": "// Parse Tier-1 model output into $json.t1 (strict-ish), while preserving everything else.\n// Works with OpenAI Responses-style output: $json.output[0].content[].text\n\nfunction getModelText(j) {\n  // Your current shape (from your example)\n  if (Array.isArray(j.output) && j.output.length) {\n    for (const msg of j.output) {\n      // msg.content is an array of parts (output_text, etc.)\n      if (Array.isArray(msg.content)) {\n        const part = msg.content.find(p =>\n          p &&\n          (p.type === 'output_text' || p.type === 'text') &&\n          typeof p.text === 'string' &&\n          p.text.trim().length > 0\n        );\n        if (part) return part.text;\n\n        // fallback: concatenate any .text fields\n        const joined = msg.content\n          .map(p => (typeof p?.text === 'string' ? p.text : ''))\n          .join('\\n')\n          .trim();\n        if (joined) return joined;\n      }\n\n      // fallback: some variants put text directly on msg\n      if (typeof msg?.text === 'string' && msg.text.trim()) return msg.text;\n    }\n  }\n\n  // Other common shapes (fallbacks)\n  return (\n    j.responseText ??\n    j.text ??\n    j.output_text ??\n    j.response ??\n    j.data ??\n    j.message?.content ??\n    j.choices?.[0]?.message?.content ??\n    j.choices?.[0]?.text ??\n    ''\n  );\n}\n\nlet s = String(getModelText($json) || '').trim();\nif (!s) throw new Error('Tier-1 parse: model output text is empty');\n\n// Strip ```json fences if present\ns = s.replace(/^```(?:json)?\\s*/i, '').replace(/\\s*```$/i, '').trim();\n\n// If there’s extra text around JSON, slice the first {...} block\nconst first = s.indexOf('{');\nconst last = s.lastIndexOf('}');\nif (first !== -1 && last !== -1 && last > first) {\n  s = s.slice(first, last + 1);\n}\n\n// Parse JSON\nlet t1;\ntry {\n  t1 = JSON.parse(s);\n} catch (e) {\n  // include a short preview to debug without dumping huge output\n  const preview = s.slice(0, 250);\n  throw new Error(`Tier-1 parse: invalid JSON. Preview: ${preview}`);\n}\n\n// Validate + normalize\nconst reqStr = (k) => typeof t1[k] === 'string' && t1[k].trim().length > 0;\n\nif (!reqStr('topic_primary')) throw new Error('Tier-1 parse: missing topic_primary');\nif (!reqStr('topic_secondary')) throw new Error('Tier-1 parse: missing topic_secondary');\nif (!reqStr('gist')) throw new Error('Tier-1 parse: missing gist');\nif (!Array.isArray(t1.keywords)) throw new Error('Tier-1 parse: keywords must be an array');\n\nt1.topic_primary = t1.topic_primary.trim();\nt1.topic_secondary = t1.topic_secondary.trim();\nt1.gist = t1.gist.trim();\n\nt1.keywords = t1.keywords\n  .map(x => String(x ?? '').trim())\n  .filter(Boolean);\n\nt1.keywords = Array.from(new Set(t1.keywords));\n\n// Keep bounds tight for your spec (5–12 is what you’ve been using)\nif (t1.keywords.length < 5) throw new Error('Tier-1 parse: keywords must have at least 5 items');\nif (t1.keywords.length > 12) t1.keywords = t1.keywords.slice(0, 12);\n\n// Confidences: clamp 0..1 if present\nconst clamp01 = (x) => (x < 0 ? 0 : x > 1 ? 1 : x);\nif (typeof t1.topic_primary_confidence === 'number') t1.topic_primary_confidence = clamp01(t1.topic_primary_confidence);\nif (typeof t1.topic_secondary_confidence === 'number') t1.topic_secondary_confidence = clamp01(t1.topic_secondary_confidence);\n\nreturn [{\n  json: {\n    ...$json,\n    t1,\n    t1_valid: true,\n    t1_raw_text: s.slice(0, 2000),\n  }\n}];\n"
      },
      "position": [
        3152,
        -480
      ],
      "type": "n8n-nodes-base.code",
      "typeVersion": 2
    },
    {
      "id": "1a6df95f-7a5c-4203-b82c-f2e0a84b70c0",
      "name": "Update T1 Info to DB",
      "parameters": {
        "jsCode": "// Update Info to DB (Tier-1): build UPDATE from $json.t1 (already parsed upstream)\n// Pattern: output { ...$json, sql } then Postgres Execute Query runs {{$json.sql}}\n\nfunction esc(s) {\n  return String(s).replace(/\\\\/g, '\\\\\\\\').replace(/'/g, \"''\");\n}\nfunction lit(v) {\n  return (v === null || v === undefined) ? 'NULL' : `'${esc(v)}'`;\n}\nfunction jsonbLit(obj) {\n  if (obj === null || obj === undefined) return 'NULL';\n  const s = JSON.stringify(obj).replace(/\\\\/g, '\\\\\\\\').replace(/'/g, \"''\");\n  return `'${s}'::jsonb`;\n}\nfunction textArrayLit(arr) {\n  if (!Array.isArray(arr) || arr.length === 0) return 'NULL';\n  const items = arr\n    .map(x => String(x ?? '').trim())\n    .filter(Boolean)\n    .map(x => `'${esc(x)}'`);\n  if (items.length === 0) return 'NULL';\n  return `ARRAY[${items.join(', ')}]::text[]`;\n}\nconst clamp01 = (x) => (x < 0 ? 0 : x > 1 ? 1 : x);\n\n// --- required identity ---\nconst id = $json.id;\nif (!id) throw new Error('Tier-1 update: missing id (merge must include INSERT RETURNING id)');\n\n// --- use parsed Tier-1 payload from previous node ---\nconst t1 = $json.t1;\nif (!t1 || typeof t1 !== 'object') {\n  throw new Error('Tier-1 update: missing $json.t1 (Parse Tier-1 node must run before this)');\n}\n\n// validate required fields (defensive)\nconst reqStr = (k) => typeof t1[k] === 'string' && t1[k].trim().length > 0;\nif (!reqStr('topic_primary')) throw new Error('Tier-1 update: missing topic_primary');\nif (!reqStr('topic_secondary')) throw new Error('Tier-1 update: missing topic_secondary');\nif (!reqStr('gist')) throw new Error('Tier-1 update: missing gist');\nif (!Array.isArray(t1.keywords)) throw new Error('Tier-1 update: keywords must be an array');\n\n// normalize\nconst topic_primary = t1.topic_primary.trim();\nconst topic_secondary = t1.topic_secondary.trim();\nconst gist = t1.gist.trim();\n\nlet keywords = t1.keywords\n  .map(x => String(x ?? '').trim())\n  .filter(Boolean);\n\nkeywords = Array.from(new Set(keywords));\nif (keywords.length < 5) throw new Error('Tier-1 update: keywords must have at least 5 items');\nif (keywords.length > 12) keywords = keywords.slice(0, 12);\n\n// confidences optional\nconst topic_primary_confidence =\n  (typeof t1.topic_primary_confidence === 'number') ? clamp01(t1.topic_primary_confidence) : null;\nconst topic_secondary_confidence =\n  (typeof t1.topic_secondary_confidence === 'number') ? clamp01(t1.topic_secondary_confidence) : null;\n\n// optional instrumentation fields\nconst enrichment_model = $json.enrichment_model ?? 'gpt-5-nano';\nconst prompt_version = $json.prompt_version ?? 'v1';\nconst saveRaw = true;\n\n// build SQL\nconst sql = `\nUPDATE pkm.entries\nSET\n  topic_primary = ${lit(topic_primary)}::text,\n  topic_primary_confidence = ${topic_primary_confidence === null ? 'NULL' : Number(topic_primary_confidence)},\n  topic_secondary = ${lit(topic_secondary)}::text,\n  topic_secondary_confidence = ${topic_secondary_confidence === null ? 'NULL' : Number(topic_secondary_confidence)},\n  keywords = ${textArrayLit(keywords)},\n  gist = ${lit(gist)}::text,\n  enrichment_status = 'done',\n  enrichment_model = ${lit(enrichment_model)}::text,\n  prompt_version = ${lit(prompt_version)}::text,\n\n  metadata = CASE\n    WHEN ${saveRaw ? 'true' : 'false'} THEN\n      jsonb_set(\n        COALESCE(metadata, '{}'::jsonb),\n        '{t1_raw}',\n        ${jsonbLit(t1)},\n        true\n      )\n    ELSE metadata\n  END\n\nWHERE id = ${lit(id)}::uuid\nRETURNING\n  entry_id,\n  id,\n  created_at,\n  source,\n  intent,\n  content_type,\n  COALESCE(title,'') AS title,\n  COALESCE(author,'') AS author,\n  COALESCE(url_canonical,'') AS url_canonical,\n  topic_primary,\n  topic_secondary,\n  gist,\n  clean_text,\n  array_length(keywords,1) AS kw_count,\n  enrichment_status;\n`.trim();\n\nreturn [{ json: { ...$json, sql } }];\n"
      },
      "position": [
        3376,
        -480
      ],
      "type": "n8n-nodes-base.code",
      "typeVersion": 2
    }
  ],
  "settings": {
    "availableInMCP": false,
    "executionOrder": "v1"
  },
  "shared": [
    {
      "createdAt": "2026-01-24T05:20:03.503Z",
      "project": {
        "createdAt": "2026-01-12T01:58:24.486Z",
        "creatorId": "d87e0db9-0ac0-46ea-9218-1eb181231d8f",
        "description": null,
        "icon": null,
        "id": "m23P41JejIbnV7Xa",
        "name": "Igor Gasovic <skljstore@gmail.com>",
        "type": "personal",
        "updatedAt": "2026-01-12T04:27:38.916Z"
      },
      "projectId": "m23P41JejIbnV7Xa",
      "role": "workflow:owner",
      "updatedAt": "2026-01-24T05:20:03.503Z",
      "workflowId": "D7ZqryewKY23am6l"
    }
  ],
  "staticData": {
    "node:Email Trigger (IMAP)1": {
      "lastMessageUid": 80
    }
  },
  "tags": [],
  "triggerCount": 1,
  "updatedAt": "2026-01-26T23:19:27.519Z",
  "versionCounter": 56
}
