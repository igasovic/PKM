{
  "active": true,
  "activeVersionId": "c5093fd5-0862-4205-bb81-20edbc549a47",
  "connections": {
    "Build SQL - INSERT": {
      "main": [
        [
          {
            "index": 0,
            "node": "Execute a SQL query",
            "type": "main"
          }
        ]
      ]
    },
    "Build SQL - UPDATE": {
      "main": [
        [
          {
            "index": 0,
            "node": "Postgres Update Extract",
            "type": "main"
          }
        ]
      ]
    },
    "Call 'PKM â€” Retrieval Config'": {
      "main": [
        [
          {
            "index": 0,
            "node": "Compute Retrieval Excerpt + Quality Signals (from capture_text)",
            "type": "main"
          }
        ]
      ]
    },
    "Compose Response": {
      "main": [
        [
          {
            "index": 0,
            "node": "Send a text message",
            "type": "main"
          }
        ]
      ]
    },
    "Compute Retrieval Excerpt + Quality Signals (from capture_text)": {
      "main": [
        [
          {
            "index": 0,
            "node": "Build SQL - INSERT",
            "type": "main"
          }
        ]
      ]
    },
    "Create Message": {
      "main": [
        [
          {
            "index": 0,
            "node": "Send a text message",
            "type": "main"
          }
        ]
      ]
    },
    "Execute a SQL query": {
      "main": [
        [
          {
            "index": 0,
            "node": "IF Has URL",
            "type": "main"
          }
        ]
      ]
    },
    "Extract Title & Author": {
      "main": [
        [
          {
            "index": 0,
            "node": "Trafilatura Extract (HTTP)1",
            "type": "main"
          },
          {
            "index": 0,
            "node": "Merge1",
            "type": "main"
          }
        ]
      ]
    },
    "HTTP Request": {
      "main": [
        [
          {
            "index": 0,
            "node": "Merge",
            "type": "main"
          }
        ]
      ]
    },
    "IF Has URL": {
      "main": [
        [
          {
            "index": 0,
            "node": "HTTP Request",
            "type": "main"
          },
          {
            "index": 1,
            "node": "Merge",
            "type": "main"
          }
        ],
        [
          {
            "index": 0,
            "node": "Compose Response",
            "type": "main"
          }
        ]
      ]
    },
    "Merge": {
      "main": [
        [
          {
            "index": 0,
            "node": "Extract Title & Author",
            "type": "main"
          }
        ]
      ]
    },
    "Merge1": {
      "main": [
        [
          {
            "index": 0,
            "node": "Text Clean",
            "type": "main"
          }
        ]
      ]
    },
    "Normalize": {
      "main": [
        [
          {
            "index": 0,
            "node": "Call 'PKM â€” Retrieval Config'",
            "type": "main"
          }
        ]
      ]
    },
    "Postgres Update Extract": {
      "main": [
        [
          {
            "index": 0,
            "node": "Create Message",
            "type": "main"
          }
        ]
      ]
    },
    "Recompute retrieval excerpt + quality signals from clean_text": {
      "main": [
        [
          {
            "index": 0,
            "node": "Build SQL - UPDATE",
            "type": "main"
          }
        ]
      ]
    },
    "Telegram Trigger": {
      "main": [
        [
          {
            "index": 0,
            "node": "Normalize",
            "type": "main"
          }
        ]
      ]
    },
    "Text Clean": {
      "main": [
        [
          {
            "index": 0,
            "node": "Recompute retrieval excerpt + quality signals from clean_text",
            "type": "main"
          }
        ]
      ]
    },
    "Trafilatura Extract (HTTP)1": {
      "main": [
        [
          {
            "index": 0,
            "node": "remove trafiltura title - hack",
            "type": "main"
          }
        ]
      ]
    },
    "When Executed by Another Workflow": {
      "main": [
        [
          {
            "index": 0,
            "node": "Normalize",
            "type": "main"
          }
        ]
      ]
    },
    "remove trafiltura title - hack": {
      "main": [
        [
          {
            "index": 1,
            "node": "Merge1",
            "type": "main"
          }
        ]
      ]
    }
  },
  "createdAt": "2026-01-13T04:28:47.800Z",
  "description": null,
  "isArchived": false,
  "name": "Telegram Capture",
  "nodes": [
    {
      "credentials": {
        "telegramApi": {
          "id": "4svco1wrzYsvoLNB",
          "name": "Telegram account"
        }
      },
      "id": "725ddc72-d79c-4041-8931-5a9a84489694",
      "name": "Send a text message",
      "parameters": {
        "additionalFields": {
          "appendAttribution": false
        },
        "chatId": "1509032341",
        "text": "={{ $json.telegram_message }}"
      },
      "position": [
        2928,
        -320
      ],
      "type": "n8n-nodes-base.telegram",
      "typeVersion": 1.2,
      "webhookId": "6f123209-5952-4fa4-b096-5a03d6e8d0f2"
    },
    {
      "credentials": {
        "postgres": {
          "id": "YICLDaLPmdddPcoA",
          "name": "Postgres account"
        }
      },
      "id": "9aaba58e-e24b-49b0-b5e6-63fbf9a28965",
      "name": "Execute a SQL query",
      "parameters": {
        "operation": "executeQuery",
        "options": {
          "queryReplacement": "="
        },
        "query": "{{$json.sql}}"
      },
      "position": [
        16,
        -320
      ],
      "type": "n8n-nodes-base.postgres",
      "typeVersion": 2.6
    },
    {
      "id": "0ff69fb7-2f88-4864-8ccd-385134a83376",
      "name": "IF Has URL",
      "parameters": {
        "conditions": {
          "combinator": "and",
          "conditions": [
            {
              "id": "144dac2d-e0d8-46f4-bd49-34d9ef783406",
              "leftValue": "={{ !!$json.url }}",
              "operator": {
                "operation": "true",
                "singleValue": true,
                "type": "boolean"
              },
              "rightValue": "true"
            }
          ],
          "options": {
            "caseSensitive": true,
            "leftValue": "",
            "typeValidation": "strict",
            "version": 1
          }
        },
        "options": {}
      },
      "position": [
        240,
        -320
      ],
      "type": "n8n-nodes-base.if",
      "typeVersion": 2
    },
    {
      "credentials": {
        "postgres": {
          "id": "YICLDaLPmdddPcoA",
          "name": "Postgres account"
        }
      },
      "id": "44d29e92-4525-4aea-bdc2-bfc782bc3d73",
      "name": "Postgres Update Extract",
      "parameters": {
        "operation": "executeQuery",
        "options": {},
        "query": "={{$json.sql}}"
      },
      "position": [
        2480,
        -416
      ],
      "type": "n8n-nodes-base.postgres",
      "typeVersion": 2
    },
    {
      "id": "ed663b2a-aae4-4856-ab00-50b04abc4f39",
      "name": "HTTP Request",
      "parameters": {
        "headerParameters": {
          "parameters": [
            {
              "name": "User-Agent",
              "value": "Mozilla/5.0 (X11; Linux aarch64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome Safari"
            },
            {
              "name": "Accept",
              "value": "text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8"
            }
          ]
        },
        "options": {
          "lowercaseHeaders": false,
          "redirect": {
            "redirect": {}
          },
          "response": {
            "response": {
              "fullResponse": true,
              "outputPropertyName": "html",
              "responseFormat": "text"
            }
          },
          "timeout": 25000
        },
        "sendHeaders": true,
        "url": "={{ $json.url_canonical }}"
      },
      "position": [
        464,
        -480
      ],
      "type": "n8n-nodes-base.httpRequest",
      "typeVersion": 4.3
    },
    {
      "id": "d652a964-3dba-422f-ba34-2c07d79fd7b5",
      "name": "Trafilatura Extract (HTTP)1",
      "parameters": {
        "options": {
          "timeout": 20000
        },
        "url": "={{'http://172.18.0.1:8787/extract?url=' + encodeURIComponent($json.url_canonical)}}"
      },
      "position": [
        1136,
        -336
      ],
      "type": "n8n-nodes-base.httpRequest",
      "typeVersion": 4
    },
    {
      "id": "bfc8a630-5489-405f-949c-49473a62f187",
      "name": "Merge",
      "parameters": {
        "combineBy": "combineByPosition",
        "mode": "combine",
        "options": {}
      },
      "position": [
        688,
        -416
      ],
      "type": "n8n-nodes-base.merge",
      "typeVersion": 3.2
    },
    {
      "id": "dddb503f-10d7-440d-9b48-ecbfef020bcf",
      "name": "Merge1",
      "parameters": {
        "combineBy": "combineByPosition",
        "mode": "combine",
        "options": {}
      },
      "position": [
        1584,
        -416
      ],
      "type": "n8n-nodes-base.merge",
      "typeVersion": 3.2
    },
    {
      "id": "a02c847f-191c-4f90-adf2-c76d3b715596",
      "name": "remove trafiltura title - hack",
      "parameters": {
        "jsCode": "const item = { ...$json };\n\n// Always discard Trafilatura title\ndelete item.title;\ndelete item.author;\n\nreturn [item];\n"
      },
      "position": [
        1360,
        -336
      ],
      "type": "n8n-nodes-base.code",
      "typeVersion": 2
    },
    {
      "id": "e7474a77-f17b-4f8f-bbe1-632804bd2e69",
      "name": "Create Message",
      "parameters": {
        "jsCode": "const entryId = ($json.entry_id ?? '').toString().trim();\n\nconst url = String($json.url_canonical || $json.url || '').trim();\nconst title = String($json.title || '').trim();\nconst author = String($json.author || '').trim();\nconst cleanLen = Number($json.clean_len || 0);\n\nconst labelBase = title || 'link';\nconst label = author ? `${labelBase} â€” ${author}` : labelBase;\n\n// Determine status purely from extracted length\nlet status = 'failed';\nif (cleanLen > 0) status = cleanLen < 500 ? 'low_quality' : 'ok';\n\nlet msg;\nconst idLine = entryId ? ` (#${entryId})` : '';\n\nif (status === 'ok') {\n  msg = `âœ… Saved${idLine}: ${label} (${cleanLen} chars)\\n${url}`;\n} else if (status === 'low_quality') {\n  msg = `âš ï¸ Saved (low quality)${idLine}: ${label} (${cleanLen} chars)\\n${url}`;\n} else {\n  msg = `âŒ Saved (extraction failed)${idLine}: ${labelBase}\\n${url}`;\n}\n\n// hard cap for Telegram\nconst MAX = 4000;\nif (msg.length > MAX) msg = msg.slice(0, MAX - 1) + 'â€¦';\n\nreturn [{ json: { ...$json, telegram_message: msg } }];\n"
      },
      "position": [
        2704,
        -416
      ],
      "type": "n8n-nodes-base.code",
      "typeVersion": 2
    },
    {
      "id": "7a117ec5-8af2-4d89-a227-128c2c488114",
      "name": "Compose Response",
      "parameters": {
        "jsCode": "const entryId = ($json.entry_id ?? '').toString().trim();\n\nconst text = String($json.capture_text || '').trim();\nconst textLen = $json.text_len;\n\n// short preview to avoid spam\nconst previewMax = 240;\nconst preview =\n  textLen > previewMax\n    ? text.slice(0, previewMax - 1) + 'â€¦'\n    : text;\n\nlet msg =\n  `ðŸ§  Thought saved` +\n  (entryId ? ` (#${entryId})` : ``) +\n  ` (${textLen} chars)\\n` +\n  `${preview}`;\n\n// hard cap for Telegram\nconst MAX = 4000;\nif (msg.length > MAX) msg = msg.slice(0, MAX - 1) + 'â€¦';\n\nreturn [{ json: { ...$json, telegram_message: msg } }];\n"
      },
      "position": [
        2704,
        -176
      ],
      "type": "n8n-nodes-base.code",
      "typeVersion": 2
    },
    {
      "id": "d0582139-b901-43a2-8fb9-b3c30be2b698",
      "name": "When Executed by Another Workflow",
      "parameters": {
        "inputSource": "passthrough"
      },
      "position": [
        -1104,
        -320
      ],
      "type": "n8n-nodes-base.executeWorkflowTrigger",
      "typeVersion": 1.1
    },
    {
      "id": "08ca99f5-2ba7-42e9-aab6-4b90e6055f85",
      "name": "Normalize",
      "parameters": {
        "jsCode": "// Telegram normalization node (fits pkm.entries schema)\n\n// 1) Get Telegram text\nconst text = $json.message?.text || '';\nconst capture_text = String(text);\n\n// 2) Extract first URL from Telegram text\nconst match = capture_text.match(/https?:\\/\\/[^\\s<>()]+/i);\nlet url = match ? match[0] : null;\n\n// Trim common trailing punctuation/brackets\nif (url) {\n  while (/[)\\],.?!:;\"'Â»]+$/.test(url)) {\n    url = url.replace(/[)\\],.?!:;\"'Â»]+$/, '');\n  }\n}\n\n// 3) Canonicalize using plain JS (no URL(), no modules)\nfunction canonicalizeUrl(raw) {\n  if (!raw) return null;\n\n  let s = String(raw).trim();\n\n  // Remove invisible characters\n  s = s.replace(/[\\u200B-\\u200D\\uFEFF]/g, '');\n\n  // Drop fragment\n  const hashIdx = s.indexOf('#');\n  if (hashIdx !== -1) s = s.slice(0, hashIdx);\n\n  // Split base / query\n  const qIdx = s.indexOf('?');\n  let base = qIdx === -1 ? s : s.slice(0, qIdx);\n  let query = qIdx === -1 ? '' : s.slice(qIdx + 1);\n\n  // Normalize scheme + host casing\n  base = base.replace(/^https?:\\/\\//i, m => m.toLowerCase());\n  base = base.replace(\n    /^https?:\\/\\/([^\\/]+)/i,\n    (m, host) => m.replace(host, host.toLowerCase())\n  );\n\n  // Remove trailing slash (except root)\n  base = base.replace(/^(https?:\\/\\/[^\\/]+)\\/+$/, '$1');\n  base = base.replace(/(.+?)\\/+$/, '$1');\n\n  // Filter tracking query params\n  if (!query) return base;\n\n  const parts = query.split('&').filter(Boolean);\n  const kept = [];\n\n  for (const part of parts) {\n    const eq = part.indexOf('=');\n    const k = eq === -1 ? part : part.slice(0, eq);\n\n    let key;\n    try { key = decodeURIComponent(k).toLowerCase(); }\n    catch { key = k.toLowerCase(); }\n\n    const drop =\n      key.startsWith('utm_') ||\n      key === 'fbclid' ||\n      key === 'gclid' ||\n      key === 'dclid' ||\n      key === 'msclkid' ||\n      key === 'igshid' ||\n      key === 'mc_cid' ||\n      key === 'mc_eid' ||\n      key === 'mkt_tok' ||\n      key === 'oly_anon_id' ||\n      key === 'oly_enc_id';\n\n    if (!drop) kept.push(part);\n  }\n\n  return kept.length ? `${base}?${kept.join('&')}` : base;\n}\n\nconst url_canonical = canonicalizeUrl(url);\n\n// 4) Guidelines -> content_type + intent\nconst content_type = url ? 'newsletter' : 'note';\nconst intent = content_type === 'newsletter' ? 'archive' : 'think';\n\n// 5) Optional: pull title/author if Telegram provides them (usually not)\nconst title =\n  $json.message?.document?.file_name ||\n  $json.message?.caption ||\n  null;\n\nconst author = null;\n\n// 6) Return normalized fields (keep original payload too)\nreturn [\n  {\n    ...$json,\n\n    // pkm.entries core fields you care about downstream\n    source: 'telegram',\n    intent,\n    content_type,\n    title,\n    author,\n\n    capture_text,\n    url,\n    url_canonical,\n  }\n];\n"
      },
      "position": [
        -880,
        -320
      ],
      "type": "n8n-nodes-base.code",
      "typeVersion": 2
    },
    {
      "id": "0cfd5e2a-ef19-4ec1-b575-eba3131c8977",
      "name": "Extract Title & Author",
      "parameters": {
        "jsCode": "// Combined TITLE + AUTHOR extraction\n// Outputs only: title, author\n\nconst html = $json.html || '';\nconst clean_text = ($json.clean_text || '').trim();\n\n// ---------------- helpers ----------------\nconst decodeEntities = (s) =>\n  String(s)\n    .replace(/&amp;/g, '&')\n    .replace(/&lt;/g, '<')\n    .replace(/&gt;/g, '>')\n    .replace(/&quot;/g, '\"')\n    .replace(/&#39;/g, \"'\")\n    .replace(/&#x27;/g, \"'\")\n    .replace(/&#x2F;/g, '/')\n    .replace(/&nbsp;/g, ' ');\n\nconst norm = (s) => {\n  if (!s) return null;\n  const t = decodeEntities(s).replace(/\\s+/g, ' ').trim();\n  return t.length ? t : null;\n};\n\nconst pickFirst = (arr) => {\n  for (const v of arr) {\n    const t = norm(v);\n    if (t) return t;\n  }\n  return null;\n};\n\n// ---------------- TITLE ----------------\nlet title = null;\n\n// 1) og:title\nlet m = html.match(\n  /<meta[^>]+property=[\"']og:title[\"'][^>]+content=[\"']([^\"']+)[\"'][^>]*>/i\n);\nif (m?.[1]) title = norm(m[1]);\n\n// 2) <title>\nif (!title) {\n  m = html.match(/<title[^>]*>([\\s\\S]*?)<\\/title>/i);\n  if (m?.[1]) title = norm(m[1]);\n}\n\n// 3) first line of clean_text\nif (!title && clean_text) {\n  const firstLine = clean_text\n    .split('\\n')\n    .map(s => s.trim())\n    .find(Boolean);\n  if (firstLine) title = firstLine.slice(0, 140);\n}\n\n// ---------------- AUTHOR ----------------\nconst metaContent = (key, attr) => {\n  const re = new RegExp(\n    `<meta[^>]+${attr}\\\\s*=\\\\s*[\"']${key}[\"'][^>]*content\\\\s*=\\\\s*[\"']([^\"']+)[\"'][^>]*>`,\n    'i'\n  );\n  const m = html.match(re);\n  return m ? m[1] : null;\n};\n\nconst titleCase = (s) =>\n  String(s)\n    .split(' ')\n    .filter(Boolean)\n    .map(w => w[0].toUpperCase() + w.slice(1))\n    .join(' ');\n\nconst authorFromValue = (val) => {\n  const v = norm(val);\n  if (!v) return null;\n\n  const parts = v.split(',').map(p => p.trim()).filter(Boolean);\n  const names = [];\n\n  for (const p of parts) {\n    if (/^https?:\\/\\//i.test(p)) {\n      const seg = p.replace(/\\/+$/, '').match(/\\/([^\\/?#]+)$/)?.[1];\n      if (!seg) continue;\n      names.push(titleCase(seg.replace(/[-_]+/g, ' ')));\n    } else {\n      names.push(p);\n    }\n  }\n\n  const seen = new Set();\n  return names\n    .filter(n => {\n      const k = n.toLowerCase();\n      if (seen.has(k)) return false;\n      seen.add(k);\n      return true;\n    })\n    .join(', ') || null;\n};\n\nconst rawAuthor = pickFirst([\n  authorFromValue(metaContent('author', 'name')),\n  authorFromValue(metaContent('parsely-author', 'name')),\n  authorFromValue(metaContent('dc.creator', 'name')),\n  authorFromValue(metaContent('dc.creator', 'property')),\n  authorFromValue(metaContent('article:author', 'property')),\n]);\n\n// Keep only the first author if multiple are present\nconst author = rawAuthor\n  ? rawAuthor.split(',').map(s => s.trim()).find(Boolean) || null\n  : null;\n\n\n// ---------------- output ----------------\nreturn [\n  {\n    json: {\n      ...$json,\n      title: title || $json.title || null,\n      author: author || $json.author || null,\n    },\n  },\n];\n"
      },
      "position": [
        912,
        -416
      ],
      "type": "n8n-nodes-base.code",
      "typeVersion": 2
    },
    {
      "id": "f9724210-2848-4b77-be35-30b5007bc3ef",
      "name": "Compute Retrieval Excerpt + Quality Signals (from capture_text)",
      "parameters": {
        "jsCode": "// WP1 Step 1A: compute retrieval excerpt + quality signals from capture_text\n// Output: $json.retrieval (object) + $json.metadata_patch ({ retrieval: ... })\n\nconst cfg = $json.config?.qualityThresholds || {};\nconst TH = {\n  excerpt_max_chars: cfg.excerpt_max_chars ?? 320,\n  low_signal: {\n    min_words: cfg.low_signal?.min_words ?? 35,\n    min_chars: cfg.low_signal?.min_chars ?? 220,\n  },\n  boilerplate: {\n    link_ratio_high: cfg.boilerplate?.link_ratio_high ?? 0.18,\n    link_count_high: cfg.boilerplate?.link_count_high ?? 25,\n  },\n  extraction_incomplete: {\n    min_extracted_chars_to_consider: cfg.extraction_incomplete?.min_extracted_chars_to_consider ?? 800,\n    clean_vs_extracted_ratio_low: cfg.extraction_incomplete?.clean_vs_extracted_ratio_low ?? 0.25,\n  },\n};\n\n// Notes should NOT be penalized just because they're short.\nfunction lowSignalThresholdFor(content_type) {\n  if (content_type === 'note') return { min_words: 8, min_chars: 40 };\n  return TH.low_signal;\n}\n\nfunction normWS(s) {\n  return String(s || '').replace(/\\s+/g, ' ').trim();\n}\n\nfunction trimUrl(u) {\n  let url = String(u || '');\n  while (/[)\\],.?!:;\"'Â»]+$/.test(url)) url = url.replace(/[)\\],.?!:;\"'Â»]+$/, '');\n  return url;\n}\n\nfunction linkCountFromText(text) {\n  const matches = String(text || '').match(/https?:\\/\\/[^\\s<>()]+/gi) || [];\n  return matches.map(trimUrl).filter(Boolean).length;\n}\n\nfunction getDomain(rawUrl) {\n  const u = String(rawUrl || '').trim();\n  const m = u.match(/^https?:\\/\\/([^\\/?#]+)/i);\n  if (!m) return null;\n  let host = m[1].toLowerCase();\n  host = host.replace(/:\\d+$/, '');      // drop port\n  host = host.replace(/^www\\./, '');     // drop www\n  return host || null;\n}\n\nfunction buildExcerpt(raw, maxChars) {\n  const s = normWS(raw);\n  if (!s) return '';\n\n  // Prefer removing URLs so excerpt isn't just a link\n  const noUrls = normWS(s.replace(/https?:\\/\\/[^\\s<>()]+/gi, ''));\n  const base = (noUrls.length >= 50) ? noUrls : s;\n\n  if (base.length <= maxChars) return base;\n\n  const cut = base.slice(0, maxChars + 1);\n  let idx = cut.lastIndexOf(' ');\n  if (idx < Math.floor(maxChars * 0.6)) idx = maxChars;\n  return base.slice(0, idx).replace(/\\s+$/g, '') + 'â€¦';\n}\n\nfunction clamp01(x) {\n  if (x < 0) return 0;\n  if (x > 1) return 1;\n  return x;\n}\n\nconst content_type = $json.content_type || 'note';\n\nconst capture_text_raw = $json.capture_text ?? '';\nconst capture_text = String(capture_text_raw);\nconst clean = normWS(capture_text);\n\nconst clean_word_count = clean ? clean.split(/\\s+/).filter(Boolean).length : 0;\nconst clean_char_count = clean.length;\n\nconst extracted_text = $json.extracted_text ?? '';\nconst extracted_char_count = String(extracted_text || '').length;\n\nconst link_count = linkCountFromText(capture_text);\nconst link_ratio = link_count / Math.max(1, clean_word_count);\n\nconst lowTH = lowSignalThresholdFor(content_type);\nconst low_signal = (clean_word_count < lowTH.min_words) || (clean_char_count < lowTH.min_chars);\n\nconst boilerplate_heavy =\n  (link_ratio > TH.boilerplate.link_ratio_high) ||\n  (link_count >= TH.boilerplate.link_count_high);\n\nconst extraction_incomplete =\n  (extracted_char_count >= TH.extraction_incomplete.min_extracted_chars_to_consider) &&\n  (clean_char_count / Math.max(1, extracted_char_count) < TH.extraction_incomplete.clean_vs_extracted_ratio_low);\n\n// Optional quality_score (0..1). Only for sorting/penalties later; not used as â€œrelevanceâ€.\nconst signal =\n  0.6 * Math.min(1, clean_word_count / 120) +\n  0.4 * Math.min(1, clean_char_count / 1200);\n\nconst penalty =\n  (boilerplate_heavy ? 0.25 : 0) +\n  (low_signal ? 0.35 : 0) +\n  (extraction_incomplete ? 0.15 : 0);\n\nconst quality_score = clamp01(signal - penalty);\n\nconst url_for_domain = $json.url_canonical || $json.url || null;\nconst source_domain = getDomain(url_for_domain);\n\nconst excerpt = buildExcerpt(capture_text, TH.excerpt_max_chars);\n\nconst retrieval = {\n  version: 'v1',\n  excerpt,\n  source_domain,\n  quality: {\n    clean_word_count,\n    clean_char_count,\n    extracted_char_count,\n    link_count,\n    link_ratio,\n    boilerplate_heavy,\n    low_signal,\n    extraction_incomplete,\n    quality_score,\n  },\n};\n\nreturn [{\n  json: {\n    ...$json,\n    retrieval,\n    metadata_patch: { retrieval },\n  }\n}];\n"
      },
      "position": [
        -432,
        -320
      ],
      "type": "n8n-nodes-base.code",
      "typeVersion": 2
    },
    {
      "id": "5116846d-f384-40e1-a928-518eb22430a7",
      "name": "Recompute retrieval excerpt + quality signals from clean_text",
      "parameters": {
        "jsCode": "// WP1 Step 1B: recompute retrieval excerpt + quality signals from clean_text\n// Output: $json.retrieval + $json.metadata_patch\n// Safe behavior: if clean_text is empty, DO NOT produce a patch.\n\nconst cfg = $json.config?.qualityThresholds || {};\nconst TH = {\n  excerpt_max_chars: cfg.excerpt_max_chars ?? 320,\n  low_signal: {\n    min_words: cfg.low_signal?.min_words ?? 35,\n    min_chars: cfg.low_signal?.min_chars ?? 220,\n  },\n  boilerplate: {\n    link_ratio_high: cfg.boilerplate?.link_ratio_high ?? 0.18,\n    link_count_high: cfg.boilerplate?.link_count_high ?? 25,\n  },\n  extraction_incomplete: {\n    min_extracted_chars_to_consider: cfg.extraction_incomplete?.min_extracted_chars_to_consider ?? 800,\n    clean_vs_extracted_ratio_low: cfg.extraction_incomplete?.clean_vs_extracted_ratio_low ?? 0.25,\n  },\n};\n\nfunction normWS(s) {\n  return String(s || '').replace(/\\s+/g, ' ').trim();\n}\n\nfunction trimUrl(u) {\n  let url = String(u || '');\n  while (/[)\\],.?!:;\"'Â»]+$/.test(url)) url = url.replace(/[)\\],.?!:;\"'Â»]+$/, '');\n  return url;\n}\n\nfunction linkCountFromText(text) {\n  const matches = String(text || '').match(/https?:\\/\\/[^\\s<>()]+/gi) || [];\n  return matches.map(trimUrl).filter(Boolean).length;\n}\n\nfunction getDomain(rawUrl) {\n  const u = String(rawUrl || '').trim();\n  const m = u.match(/^https?:\\/\\/([^\\/?#]+)/i);\n  if (!m) return null;\n  let host = m[1].toLowerCase();\n  host = host.replace(/:\\d+$/, '');\n  host = host.replace(/^www\\./, '');\n  return host || null;\n}\n\nfunction buildExcerpt(raw, maxChars) {\n  const s = normWS(raw);\n  if (!s) return '';\n  const noUrls = normWS(s.replace(/https?:\\/\\/[^\\s<>()]+/gi, ''));\n  const base = (noUrls.length >= 50) ? noUrls : s;\n  if (base.length <= maxChars) return base;\n  const cut = base.slice(0, maxChars + 1);\n  let idx = cut.lastIndexOf(' ');\n  if (idx < Math.floor(maxChars * 0.6)) idx = maxChars;\n  return base.slice(0, idx).replace(/\\s+$/g, '') + 'â€¦';\n}\n\nfunction clamp01(x) {\n  if (x < 0) return 0;\n  if (x > 1) return 1;\n  return x;\n}\n\nconst clean_text_raw = $json.clean_text ?? '';\nconst clean_text = String(clean_text_raw);\nconst clean = normWS(clean_text);\n\nif (!clean) {\n  // Do not overwrite metadata if extraction failed / empty.\n  return [{ json: { ...$json, retrieval_update_skipped: true } }];\n}\n\nconst clean_word_count = clean.split(/\\s+/).filter(Boolean).length;\nconst clean_char_count = clean.length;\n\nconst extracted_text = $json.extracted_text ?? '';\nconst extracted_char_count = String(extracted_text || '').length;\n\nconst link_count = linkCountFromText(clean_text);\nconst link_ratio = link_count / Math.max(1, clean_word_count);\n\nconst low_signal = (clean_word_count < TH.low_signal.min_words) || (clean_char_count < TH.low_signal.min_chars);\n\nconst boilerplate_heavy =\n  (link_ratio > TH.boilerplate.link_ratio_high) ||\n  (link_count >= TH.boilerplate.link_count_high);\n\nconst extraction_incomplete =\n  (extracted_char_count >= TH.extraction_incomplete.min_extracted_chars_to_consider) &&\n  (clean_char_count / Math.max(1, extracted_char_count) < TH.extraction_incomplete.clean_vs_extracted_ratio_low);\n\nconst signal =\n  0.6 * Math.min(1, clean_word_count / 120) +\n  0.4 * Math.min(1, clean_char_count / 1200);\n\nconst penalty =\n  (boilerplate_heavy ? 0.25 : 0) +\n  (low_signal ? 0.35 : 0) +\n  (extraction_incomplete ? 0.15 : 0);\n\nconst quality_score = clamp01(signal - penalty);\n\nconst url_for_domain = $json.url_canonical || $json.url || null;\nconst source_domain = getDomain(url_for_domain);\n\nconst excerpt = buildExcerpt(clean_text, TH.excerpt_max_chars);\n\nconst retrieval = {\n  version: 'v1',\n  excerpt,\n  source_domain,\n  quality: {\n    clean_word_count,\n    clean_char_count,\n    extracted_char_count,\n    link_count,\n    link_ratio,\n    boilerplate_heavy,\n    low_signal,\n    extraction_incomplete,\n    quality_score,\n  },\n};\n\nreturn [{\n  json: {\n    ...$json,\n    retrieval,\n    metadata_patch: { retrieval },\n  }\n}];\n"
      },
      "position": [
        2032,
        -416
      ],
      "type": "n8n-nodes-base.code",
      "typeVersion": 2
    },
    {
      "id": "5ea800e9-24b0-4674-8ec9-e0a92e5c574b",
      "name": "Build SQL - INSERT",
      "parameters": {
        "jsCode": "function sqlString(v) {\n  if (v === null || v === undefined) return 'NULL';\n  const s = String(v).replace(/\\\\/g, '\\\\\\\\').replace(/'/g, \"''\");\n  return `'${s}'`;\n}\n\nfunction sqlJsonb(obj) {\n  if (obj === null || obj === undefined) return 'NULL';\n  const s = JSON.stringify(obj).replace(/\\\\/g, '\\\\\\\\').replace(/'/g, \"''\");\n  return `'${s}'::jsonb`;\n}\n\nfunction sqlInt(v) {\n  if (v === null || v === undefined) return 'NULL';\n  const n = Number(v);\n  if (!Number.isFinite(n)) return 'NULL';\n  return String(Math.trunc(n));\n}\n\nfunction sqlNum(v) {\n  if (v === null || v === undefined) return 'NULL';\n  const n = Number(v);\n  if (!Number.isFinite(n)) return 'NULL';\n  return String(n);\n}\n\nfunction sqlBool(v) {\n  if (v === null || v === undefined) return 'NULL';\n  return v ? 'true' : 'false';\n}\n\nconst msg = $json.message || {};\n\nconst source = 'telegram';\nconst intent = $json.intent || 'archive';\nconst content_type = $json.content_type || null;\n\nconst title = $json.title ?? null;\nconst author = $json.author ?? null;\n\nconst capture_text = $json.capture_text ?? msg.text ?? '';\nconst url = $json.url ?? null;\nconst url_canonical = $json.url_canonical ?? null;\n\n// metadata + retrieval (WP1 compute node)\nconst metadata_patch = $json.metadata_patch ?? ($json.retrieval ? { retrieval: $json.retrieval } : null);\nconst r = $json.retrieval ?? metadata_patch?.retrieval ?? null;\nconst q = r?.quality ?? {};\n\n// promoted retrieval columns (WP2)\nconst retrieval_excerpt = r?.excerpt ?? null;\nconst retrieval_version = r?.version ?? null;\nconst source_domain = r?.source_domain ?? null;\n\nconst clean_word_count = q.clean_word_count ?? null;\nconst clean_char_count = q.clean_char_count ?? null;\nconst extracted_char_count = q.extracted_char_count ?? null;\n\nconst link_count = q.link_count ?? null;\nconst link_ratio = q.link_ratio ?? null;\n\nconst boilerplate_heavy = q.boilerplate_heavy ?? null;\nconst low_signal = q.low_signal ?? null;\nconst extraction_incomplete = q.extraction_incomplete ?? null;\n\nconst quality_score = q.quality_score ?? null;\n\nconst sql = `\nINSERT INTO pkm.entries (\n  created_at,\n  source,\n  intent,\n  content_type,\n  title,\n  author,\n  capture_text,\n  url,\n  url_canonical,\n  metadata,\n\n  -- WP2 promoted retrieval columns\n  retrieval_excerpt,\n  retrieval_version,\n  source_domain,\n  clean_word_count,\n  clean_char_count,\n  extracted_char_count,\n  link_count,\n  link_ratio,\n  boilerplate_heavy,\n  low_signal,\n  extraction_incomplete,\n  quality_score\n)\nVALUES (\n  now(),\n  ${sqlString(source)}::text,\n  ${sqlString(intent)}::text,\n  ${sqlString(content_type)}::text,\n  ${sqlString(title)}::text,\n  ${sqlString(author)}::text,\n  ${sqlString(capture_text)}::text,\n  ${sqlString(url)}::text,\n  ${sqlString(url_canonical)}::text,\n  ${sqlJsonb(metadata_patch)},\n\n  ${sqlString(retrieval_excerpt)}::text,\n  ${sqlString(retrieval_version)}::text,\n  ${sqlString(source_domain)}::text,\n  ${sqlInt(clean_word_count)}::int,\n  ${sqlInt(clean_char_count)}::int,\n  ${sqlInt(extracted_char_count)}::int,\n  ${sqlInt(link_count)}::int,\n  ${sqlNum(link_ratio)}::real,\n  ${sqlBool(boilerplate_heavy)}::boolean,\n  ${sqlBool(low_signal)}::boolean,\n  ${sqlBool(extraction_incomplete)}::boolean,\n  ${sqlNum(quality_score)}::real\n)\nRETURNING\n  entry_id,\n  id,\n  created_at,\n  source,\n  intent,\n  content_type,\n  title,\n  author,\n  url,\n  url_canonical,\n  COALESCE(char_length(capture_text), 0) AS text_len;\n`.trim();\n\nreturn [{ json: { ...$json, sql } }];\n"
      },
      "position": [
        -208,
        -320
      ],
      "type": "n8n-nodes-base.code",
      "typeVersion": 2
    },
    {
      "id": "1c1e479b-b8f6-4d85-9c69-8c0f9943982f",
      "name": "Build SQL - UPDATE",
      "parameters": {
        "jsCode": "// BUILD SQL UPDATE (safe clean_text + extracted_text + metadata->retrieval merge + WP2 promoted retrieval columns)\n\n// ---- helpers (MUST be before use) ----\nconst esc = (s) => String(s).replace(/\\\\/g, '\\\\\\\\').replace(/'/g, \"''\");\nconst lit = (v) => (v === null || v === undefined) ? 'NULL' : `'${esc(v)}'`;\nconst jsonbLit = (obj) => {\n  if (!obj) return 'NULL';\n  const s = JSON.stringify(obj).replace(/\\\\/g, '\\\\\\\\').replace(/'/g, \"''\");\n  return `'${s}'::jsonb`;\n};\nconst intLit = (v) => {\n  if (v === null || v === undefined) return 'NULL';\n  const n = Number(v);\n  if (!Number.isFinite(n)) return 'NULL';\n  return String(Math.trunc(n));\n};\nconst numLit = (v) => {\n  if (v === null || v === undefined) return 'NULL';\n  const n = Number(v);\n  if (!Number.isFinite(n)) return 'NULL';\n  return String(n);\n};\nconst boolLit = (v) => {\n  if (v === null || v === undefined) return 'NULL';\n  return v ? 'true' : 'false';\n};\n\n// ---- required identity ----\nconst id = $json.id;\n\n// ---- urls ----\nconst url = $json.url ?? null;\nconst url_canonical = $json.url_canonical ?? null;\n\n// ---- extracted fields ----\nconst title = $json.title ?? null;\nconst author = $json.author ?? null;\n\n// ---- core content ----\nconst clean_text = String($json.clean_text ?? '');\nconst clean_trim = clean_text.trim();\nconst cleanLit = clean_trim ? `${lit(clean_text)}::text` : 'NULL';\n\n// ---- extracted_text ----\nconst extracted_raw = $json.extracted_text ?? $json.text ?? null;\nconst extracted_text = (extracted_raw === null || extracted_raw === undefined) ? null : String(extracted_raw);\nconst extracted_trim = extracted_text ? extracted_text.trim() : '';\nconst extractedLit = extracted_trim ? `${lit(extracted_text)}::text` : 'NULL';\n\n// ---- retrieval patch ----\nconst retrieval = $json.retrieval ?? null;\nconst doMeta = !!(retrieval && typeof retrieval === 'object');\nconst q = doMeta ? (retrieval.quality || {}) : {};\n\n// promoted retrieval columns (WP2)\nconst retrieval_excerpt = doMeta ? (retrieval.excerpt ?? null) : null;\nconst retrieval_version = doMeta ? (retrieval.version ?? null) : null;\nconst source_domain = doMeta ? (retrieval.source_domain ?? null) : null;\n\nconst clean_word_count = doMeta ? (q.clean_word_count ?? null) : null;\nconst clean_char_count = doMeta ? (q.clean_char_count ?? null) : null;\nconst extracted_char_count = doMeta ? (q.extracted_char_count ?? null) : null;\n\nconst link_count = doMeta ? (q.link_count ?? null) : null;\nconst link_ratio = doMeta ? (q.link_ratio ?? null) : null;\n\nconst boilerplate_heavy = doMeta ? (q.boilerplate_heavy ?? null) : null;\nconst low_signal = doMeta ? (q.low_signal ?? null) : null;\nconst extraction_incomplete = doMeta ? (q.extraction_incomplete ?? null) : null;\n\nconst quality_score = doMeta ? (q.quality_score ?? null) : null;\n\n// ---- SQL ----\nconst sql = `\nUPDATE pkm.entries\nSET\n  url = COALESCE(${lit(url)}, url),\n  url_canonical = COALESCE(${lit(url_canonical)}, url_canonical),\n\n  -- only overwrite when non-empty\n  clean_text = COALESCE(${cleanLit}, clean_text),\n\n  -- only overwrite when non-empty\n  extracted_text = COALESCE(${extractedLit}, extracted_text),\n\n  title = COALESCE(${lit(title)}::text, title),\n  author = COALESCE(${lit(author)}::text, author),\n\n  metadata = CASE\n    WHEN ${doMeta ? 'true' : 'false'} THEN\n      jsonb_set(\n        COALESCE(metadata, '{}'::jsonb),\n        '{retrieval}',\n        ${jsonbLit(retrieval)},\n        true\n      )\n    ELSE metadata\n  END,\n\n  -- WP2 promoted retrieval columns: update only when retrieval exists\n  retrieval_excerpt = CASE WHEN ${doMeta ? 'true' : 'false'} THEN ${lit(retrieval_excerpt)}::text ELSE retrieval_excerpt END,\n  retrieval_version = CASE WHEN ${doMeta ? 'true' : 'false'} THEN ${lit(retrieval_version)}::text ELSE retrieval_version END,\n  source_domain = CASE WHEN ${doMeta ? 'true' : 'false'} THEN ${lit(source_domain)}::text ELSE source_domain END,\n\n  clean_word_count = CASE WHEN ${doMeta ? 'true' : 'false'} THEN ${intLit(clean_word_count)}::int ELSE clean_word_count END,\n  clean_char_count = CASE WHEN ${doMeta ? 'true' : 'false'} THEN ${intLit(clean_char_count)}::int ELSE clean_char_count END,\n  extracted_char_count = CASE WHEN ${doMeta ? 'true' : 'false'} THEN ${intLit(extracted_char_count)}::int ELSE extracted_char_count END,\n\n  link_count = CASE WHEN ${doMeta ? 'true' : 'false'} THEN ${intLit(link_count)}::int ELSE link_count END,\n  link_ratio = CASE WHEN ${doMeta ? 'true' : 'false'} THEN ${numLit(link_ratio)}::real ELSE link_ratio END,\n\n  boilerplate_heavy = CASE WHEN ${doMeta ? 'true' : 'false'} THEN ${boolLit(boilerplate_heavy)}::boolean ELSE boilerplate_heavy END,\n  low_signal = CASE WHEN ${doMeta ? 'true' : 'false'} THEN ${boolLit(low_signal)}::boolean ELSE low_signal END,\n  extraction_incomplete = CASE WHEN ${doMeta ? 'true' : 'false'} THEN ${boolLit(extraction_incomplete)}::boolean ELSE extraction_incomplete END,\n\n  quality_score = CASE WHEN ${doMeta ? 'true' : 'false'} THEN ${numLit(quality_score)}::real ELSE quality_score END,\n\n  content_hash = NULL\nWHERE id = ${lit(id)}::uuid\nRETURNING\n  entry_id,\n  id,\n  created_at,\n  source,\n  intent,\n  content_type,\n  COALESCE(title,'') AS title,\n  COALESCE(author,'') AS author,\n  url_canonical,\n  COALESCE(char_length(clean_text), 0) AS clean_len,\n  COALESCE(char_length(extracted_text), 0) AS extracted_len;\n`.trim();\n\nreturn [{ json: { ...$json, sql } }];\n"
      },
      "position": [
        2256,
        -416
      ],
      "type": "n8n-nodes-base.code",
      "typeVersion": 2
    },
    {
      "id": "ef9f91cc-dbd4-48b7-a22d-66f47ea07239",
      "name": "Text Clean",
      "parameters": {
        "jsCode": "const cleanText = (s) => {\n  if (!s) return '';\n\n  let t = String(s);\n\n  // Remove zero-width + BOM\n  t = t.replace(/[\\u200B-\\u200D\\uFEFF]/g, '');\n\n  // Normalize line endings\n  t = t.replace(/\\r\\n/g, '\\n').replace(/\\r/g, '\\n');\n\n  // Convert non-breaking spaces to normal spaces\n  t = t.replace(/\\u00A0/g, ' ');\n\n  // Trim lines and collapse blank lines\n  const lines = t.split('\\n').map(l => l.trim());\n  const out = [];\n  let prevBlank = false;\n\n  for (const line of lines) {\n    const blank = line.length === 0;\n    if (blank) {\n      if (!prevBlank) out.push('');\n      prevBlank = true;\n    } else {\n      out.push(line);\n      prevBlank = false;\n    }\n  }\n\n  t = out.join('\\n').trim();\n\n  // Collapse excessive spaces inside lines\n  t = t.replace(/[ \\t]+/g, ' ');\n\n  return t;\n};\n\n// Trafilatura output\nconst extracted = $json.text || '';\nconst clean_text = cleanText(extracted);\n\nreturn [\n  {\n    ...$json,\n    extracted_text: extracted,              // <-- NEW alias for downstream\n    extracted_len: String(extracted).length, // <-- optional debug\n    clean_text,\n    clean_len: clean_text.length,\n  }\n];\n"
      },
      "position": [
        1808,
        -416
      ],
      "type": "n8n-nodes-base.code",
      "typeVersion": 2
    },
    {
      "id": "8c21d27b-1a45-46d8-bcaf-976265c0ab96",
      "name": "Call 'PKM â€” Retrieval Config'",
      "parameters": {
        "options": {
          "waitForSubWorkflow": true
        },
        "workflowId": {
          "__rl": true,
          "cachedResultName": "PKM â€” Retrieval Config",
          "cachedResultUrl": "/workflow/Ue-cGR6_WP4Fe5C19tA28",
          "mode": "list",
          "value": "Ue-cGR6_WP4Fe5C19tA28"
        },
        "workflowInputs": {
          "attemptToConvertTypes": false,
          "convertFieldsToString": true,
          "mappingMode": "defineBelow",
          "matchingColumns": [],
          "schema": [],
          "value": {}
        }
      },
      "position": [
        -656,
        -320
      ],
      "type": "n8n-nodes-base.executeWorkflow",
      "typeVersion": 1.3
    },
    {
      "credentials": {
        "telegramApi": {
          "id": "4svco1wrzYsvoLNB",
          "name": "Telegram account"
        }
      },
      "disabled": true,
      "id": "613e4f79-31d5-4a3b-9266-ccf4d3cecb0f",
      "name": "Telegram Trigger",
      "parameters": {
        "additionalFields": {
          "chatIds": "1509032341"
        },
        "updates": [
          "message"
        ]
      },
      "position": [
        -1104,
        -480
      ],
      "type": "n8n-nodes-base.telegramTrigger",
      "typeVersion": 1.2,
      "webhookId": "a111da0e-d82b-4eac-9ca8-ddd49cc09794"
    }
  ],
  "settings": {
    "availableInMCP": false,
    "executionOrder": "v1"
  },
  "shared": [
    {
      "createdAt": "2026-01-13T04:28:47.800Z",
      "project": {
        "createdAt": "2026-01-12T01:58:24.486Z",
        "creatorId": "d87e0db9-0ac0-46ea-9218-1eb181231d8f",
        "description": null,
        "icon": null,
        "id": "m23P41JejIbnV7Xa",
        "name": "Igor Gasovic <skljstore@gmail.com>",
        "type": "personal",
        "updatedAt": "2026-01-12T04:27:38.916Z"
      },
      "projectId": "m23P41JejIbnV7Xa",
      "role": "workflow:owner",
      "updatedAt": "2026-01-13T04:28:47.800Z",
      "workflowId": "EWyb1cTmqDlKY2pIyqULN"
    }
  ],
  "staticData": null,
  "tags": [],
  "triggerCount": 0,
  "updatedAt": "2026-01-26T23:14:42.001Z",
  "versionCounter": 127
}
